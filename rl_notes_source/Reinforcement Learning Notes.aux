\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\providecommand\AtEndDvi@Check{}
\AtEndDvi@Check
\HyPL@Entry{0<</S/D>>}
\providecommand\tcolorbox@label[2]{}
\citation{sutton2018reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}}
\citation{HB}
\citation{robert2013monte}
\@writefile{toc}{\contentsline {section}{\numberline {2}Review of Basic Probability}{5}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Interpretation of Probability}{5}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Transformations}{5}{subsection.2.2}}
\newlabel{eq_trans_variables}{{2.2}{5}{Transformations}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Limit Theorem}{5}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Sampling \& Monte Carlo Methods}{6}{subsection.2.4}}
\newlabel{eq_monte_carlo_int}{{2.4}{7}{Sampling \& Monte Carlo Methods}{subsection.2.4}{}}
\newlabel{eq_importance_sampling}{{2.4}{7}{Sampling \& Monte Carlo Methods}{subsection.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Basic Inequalities}{8}{subsection.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Concentration Inequalities}{10}{subsection.2.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Conditional Expectation}{12}{subsection.2.7}}
\citation{si252}
\citation{sutton2018reinforcement}
\citation{slivkins2019introduction}
\citation{lattimore2018bandit}
\citation{liblog}
\@writefile{toc}{\contentsline {section}{\numberline {3}Bandit Algorithms}{14}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Bandit Models}{14}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Stochastic Bandits}{14}{subsection.3.2}}
\newlabel{rl_regret}{{3.2}{14}{Stochastic Bandits}{subsection.3.2}{}}
\newlabel{rl_regret_count}{{3.2}{15}{Stochastic Bandits}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Greedy Algorithms}{15}{subsection.3.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces $\varepsilon -$greedy\relax }}{16}{algorithm.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg: eps}{{1}{16}{$\varepsilon -$greedy\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}UCB Algorithms}{16}{subsection.3.4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces UCB1\relax }}{17}{algorithm.2}}
\newlabel{alg: ucb}{{2}{17}{UCB1\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Thompson Sampling Algorithms}{17}{subsection.3.5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Thompson Sampling\relax }}{18}{algorithm.3}}
\newlabel{alg: ts}{{3}{18}{Thompson Sampling\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Gradient Bandit Algorithms}{18}{subsection.3.6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Gradient Bandit\relax }}{19}{algorithm.4}}
\newlabel{alg: gb}{{4}{19}{Gradient Bandit\relax }{algorithm.4}{}}
\citation{si252}
\citation{HB}
\citation{robert2013monte}
\@writefile{toc}{\contentsline {section}{\numberline {4}Markov Chains}{20}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Markov Model}{20}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Basic Computations}{20}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Classifications}{21}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Stationary Distribution}{22}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Reversibility}{22}{subsection.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Markov Chain Monte Carlo}{23}{subsection.4.6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Metropolis-Hastings\relax }}{24}{algorithm.5}}
\newlabel{alg: Metropolis-Hastings}{{5}{24}{Metropolis-Hastings\relax }{algorithm.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Gibbs Sampling\relax }}{24}{algorithm.6}}
\newlabel{alg: Gibbs Sampling}{{6}{24}{Gibbs Sampling\relax }{algorithm.6}{}}
\citation{si252}
\citation{introRL}
\citation{ucl_rl}
\citation{sutton2018reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {5}Markov Decision Process}{25}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Markov Reward Process}{25}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Markov Decision Process}{26}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Dynamic Programming}{28}{subsection.5.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Policy Evaluation}{28}{subsubsection.5.3.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Iterative Policy Evaluation\relax }}{29}{algorithm.7}}
\newlabel{alg: Iterative Policy Evaluation}{{7}{29}{Iterative Policy Evaluation\relax }{algorithm.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Policy Iteration}{29}{subsubsection.5.3.2}}
\newlabel{sec: PI}{{5.3.2}{29}{Policy Iteration}{subsubsection.5.3.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces Policy Iteration\relax }}{30}{algorithm.8}}
\newlabel{alg: Policy Iteration}{{8}{30}{Policy Iteration\relax }{algorithm.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Value Iteration}{30}{subsubsection.5.3.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces Value Iteration\relax }}{31}{algorithm.9}}
\newlabel{alg: Value Iteration}{{9}{31}{Value Iteration\relax }{algorithm.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.4}Comparison}{32}{subsubsection.5.3.4}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Dynamic Programming algorithms for MDPs\relax }}{32}{table.caption.1}}
\newlabel{dp_comp}{{1}{32}{Dynamic Programming algorithms for MDPs\relax }{table.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Model-Free Prediction}{33}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Monte-Carlo Policy Evaluation}{33}{subsection.6.1}}
\newlabel{subsec: Monte-Carlo Policy Evaluation}{{6.1}{33}{Monte-Carlo Policy Evaluation}{subsection.6.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {10}{\ignorespaces First-Visit Monte-Carlo Policy Evaluation\relax }}{34}{algorithm.10}}
\newlabel{alg: First-Visit Monte-Carlo Policy Evaluation}{{10}{34}{First-Visit Monte-Carlo Policy Evaluation\relax }{algorithm.10}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {11}{\ignorespaces Every-Visit Monte-Carlo Policy Evaluation\relax }}{34}{algorithm.11}}
\newlabel{alg: Every-Visit Monte-Carlo Policy Evaluation}{{11}{34}{Every-Visit Monte-Carlo Policy Evaluation\relax }{algorithm.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Temporal-Difference Learning}{35}{subsection.6.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {12}{\ignorespaces TD(0) Evaluation\relax }}{35}{algorithm.12}}
\newlabel{alg: TD(0) Evaluation}{{12}{35}{TD(0) Evaluation\relax }{algorithm.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textit  {Bootstrapping} and \textit  {sampling} for DP, MC, and TD.\relax }}{36}{table.caption.2}}
\newlabel{boot_samp}{{2}{36}{\textit {Bootstrapping} and \textit {sampling} for DP, MC, and TD.\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Model-Free Control}{37}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}On Policy Monte-Carlo Control}{37}{subsection.7.1}}
\citation{sutton2018reinforcement}
\@writefile{loa}{\contentsline {algorithm}{\numberline {13}{\ignorespaces Monte-Carlo Control\relax }}{38}{algorithm.13}}
\newlabel{alg: Monte-Carlo Control}{{13}{38}{Monte-Carlo Control\relax }{algorithm.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}On Policy Temporal-Difference Control: Sarsa}{39}{subsection.7.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {14}{\ignorespaces TD Sarsa\relax }}{39}{algorithm.14}}
\newlabel{alg: TD Sarsa}{{14}{39}{TD Sarsa\relax }{algorithm.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Off-Policy Temporal-Difference Control: Q-Learning}{40}{subsection.7.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {15}{\ignorespaces Q-Learning\relax }}{40}{algorithm.15}}
\newlabel{alg: Q-Learning}{{15}{40}{Q-Learning\relax }{algorithm.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Value Function Approximation}{41}{section.8}}
\newlabel{sec: value function approximation}{{8}{41}{Value Function Approximation}{section.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Semi-gradient Method}{41}{subsection.8.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1}Semi-gradient Method for Prediction}{42}{subsubsection.8.1.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {16}{\ignorespaces Gradient Monte Carlo Policy Evaluation\relax }}{42}{algorithm.16}}
\newlabel{alg: Gradient Monte-Carlo Policy Evaluation}{{16}{42}{Gradient Monte Carlo Policy Evaluation\relax }{algorithm.16}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {17}{\ignorespaces Semi-gradient TD(0) Policy Evaluation\relax }}{42}{algorithm.17}}
\newlabel{alg: Semi-gradient TD(0) Policy Evaluation}{{17}{42}{Semi-gradient TD(0) Policy Evaluation\relax }{algorithm.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2}Semi-gradient Method for Control}{42}{subsubsection.8.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Deep Q-Learning}{43}{subsection.8.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {18}{\ignorespaces Semi-gradient TD(0) Policy Evaluation\relax }}{44}{algorithm.18}}
\newlabel{alg: Semi-gradient TD(0) Policy Evaluation}{{18}{44}{Semi-gradient TD(0) Policy Evaluation\relax }{algorithm.18}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {19}{\ignorespaces Deep-Q Network\relax }}{45}{algorithm.19}}
\newlabel{alg: Deep-Q Network}{{19}{45}{Deep-Q Network\relax }{algorithm.19}{}}
\newlabel{sub:deep_q_networks}{{8.2}{45}{Deep Q-Learning}{algorithm.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Policy Optimization}{46}{section.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Policy Optimization Theorem}{46}{subsection.9.1}}
\citation{sutton2018reinforcement}
\citation{liblog-pga}
\citation{schulman2015highdimensional}
\citation{notes_GAE}
\citation{liblog-pga}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}REINFORCE: Monte-Carlo Policy Gradient}{49}{subsection.9.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {20}{\ignorespaces REINFORCE: Monte-Carlo Policy-Gradient Control\relax }}{50}{algorithm.20}}
\newlabel{alg: REINFORCE: Monte-Carlo Policy-Gradient Control}{{20}{50}{REINFORCE: Monte-Carlo Policy-Gradient Control\relax }{algorithm.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Actor-Critic Policy Gradient}{51}{subsection.9.3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {21}{\ignorespaces Actor-Critic\relax }}{51}{algorithm.21}}
\newlabel{alg: Actor-Critic}{{21}{51}{Actor-Critic\relax }{algorithm.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Extension of Policy Gradient}{52}{subsection.9.4}}
\bibstyle{IEEEtran}
\bibdata{re}
\bibcite{sutton2018reinforcement}{1}
\bibcite{HB}{2}
\bibcite{robert2013monte}{3}
\bibcite{si252}{4}
\bibcite{slivkins2019introduction}{5}
\bibcite{lattimore2018bandit}{6}
\bibcite{liblog}{7}
\bibcite{introRL}{8}
\bibcite{ucl_rl}{9}
\bibcite{liblog-pga}{10}
\bibcite{schulman2015highdimensional}{11}
\bibcite{notes_GAE}{12}
\zref@newlabel{LastPage}{\default{9.4}\page{53}\abspage{53}}
