%!TEX program = xelatex
% !TEX root = Reinforcement Learning Notes.tex

\documentclass{progartcn}

\usepackage{amsmath,mathrsfs,amsfonts,amsthm,amssymb}
%\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{algorithm,algpseudocode}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage[justification=centering]{caption}
\usepackage{enumitem}
\usepackage{fontawesome}
\usepackage{float}
\usepackage{footnote}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{listings}
\usepackage{pdfpages}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{tcolorbox}
\usepackage{wrapfig}
\usepackage{ulem} % \uline
\usepackage[dvipsnames]{xcolor}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newcommand{\algmargin}{\the\ALG@thistlm}
\makeatother
\newlength{\whilewidth}
\settowidth{\whilewidth}{\algorithmicwhile\ }
\algdef{SE}[parWHILE]{parWhile}{EndparWhile}[1]
  {\parbox[t]{\dimexpr\linewidth-\algmargin}{%
     \hangindent\whilewidth\strut\algorithmicwhile\ #1\ \algorithmicdo\strut}}{\algorithmicend\ \algorithmicwhile}%
\algnewcommand{\parState}[1]{\State%
  \parbox[t]{\dimexpr\linewidth-\algmargin}{\strut #1\strut}}

\algdef{SE}[parIF]{parIf}{EndparIf}[1]
  {\parbox[t]{\dimexpr\linewidth-\algmargin}{%
     \hangindent\ifwidth\strut\algorithmicif\ #1\ \algorithmicdo\strut}}{\algorithmicend\ \algorithmicif}%

\title{\bfseries\sffamily 
  Reinforcement Learning \\ % 文档主标题
  \normalfont\zihao{-3}
    \textbf{An Introductory Note}% 文档副标题
    %\textbf{Notes}
}
\author{Jingye Wang \\ \faEnvelope~ wangjy5@shanghaitech.edu.cn}  %模版源：https://github.com/WisdomFusion
\date{Spring 2020}


\begin{document}

\sloppy % 解决中英文混排文字超出边界问题


\maketitle
\thispagestyle{empty}
\renewcommand{\refname}{References}
\renewcommand{\contentsname}{Contents}

\begin{comment}
\noindent\rule[0.25\baselineskip]{\textwidth}{1pt}

	The framework of this note is enlightened by \textit{SI-252}~\cite{si252}, while the contents mainly derived from not only \textit{SI-252} but also \textit{Intro to Reinforcement Learning, Bolei Zhou}~\cite{introRL}, \textit{Reinforcement Learning, David Silver}~\cite{ucl_rl}, \textit{etc}. Many thanks to these great works!
\end{comment}

\noindent\rule[0.25\baselineskip]{\textwidth}{1pt}


\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{4}
\tableofcontents

\pagebreak

\section{Introduction}

	Course Prerequisite:
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Linear Algebra
		\item Probability
		\item Machine Learning relevant course (data mining, pattern recognition, \textit{etc})
		\item PyTorch, Python\\
	\end{itemize}

	What is Reinforcement Learning and why we care:

	A computational approach to learning whereby \textit{an agent} tries to \textit{maximize} the total amount of \textit{reward} it receives while interacting with a complex and uncertain \textit{environment}.\cite{sutton2018reinforcement}\\

	Differences between Reinforcement Learning and Supervised Learning:
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Sequential data as input (\textit{not i.i.d});
		\item The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them;
		\item \textit{Trial-and-error} exploration (balance between exploration and exploitation);
		\item There is \textit{no supervisor}, only a reward signal, which is also \textit{delayed}\\
	\end{itemize}

	Big deal: able to achieve superhuman performance
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Upper bound for Supervised Learning is human-performance. 
		\item Upper bound for Reinforcement Learning ?\\
	\end{itemize}

	Why Reinforcement Learning works now?
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Computation power: many GPUs to do trial-and-error rollout;
		\item Acquire the high degree of proficiency in domains governed by simple, known rules;
		\item End-to-end training, features and policy are jointly optimized toward the end goal.\\
	\end{itemize}

	Sequential Decision Making:
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Agent and Environment: the agent learns to interact with the environment;
		\item Rewards: a scalar feedback signal that indicates how well agent is doing;
		\item Policy: a map function from state/observation to action models the agent’s behavior;
		\item Value function: expected discounted sum of future rewards under a particular policy;
		\item Objective of the agent: selects a series of actions to maximize total future rewards;
		\item History: a sequence of observations, actions, rewards;
		\item Full observability: agent directly observes the environment state, formally as Markov decision process (MDP);
		\item Partial observability: agent indirectly observes the environment, formally as partially observable Markov decision process (POMDP)
	\end{itemize}

	All goals of the agent can be described by the maximization of expected cumulative reward.\\

	Types of Reinforcement Learning agents based on What the Agent Learns
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Value-based agent:
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Explicit: Value function;
			\item Implicit: Policy (can derive a policy from value function);
		\end{itemize}
		\item Policy-based agent: 
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Explicit: policy;
			\item No value function;
		\end{itemize}
		\item Actor-Critic agent:
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Explicit: policy and value function.\\
		\end{itemize}
	\end{itemize}

	Types of Reinforcement Learning agents on if there is model
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Model-based:
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Explicit: model;
			\item May or may not have policy and/or value function;
		\end{itemize}
		\item Model-free: 
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Explicit: value function and/or policy function;
			\item No model.
		\end{itemize}
	\end{itemize}

\pagebreak

\section{Review of Basic Probability}

	For more details of this part one can refer to \textit{Introduction to Probability}~\cite{HB} and \textit{Monte Carlo Statistical Methods}~\cite{robert2013monte}.\\

	\subsection{Interpretation of Probability}

		\textbf{The Frequentist view}: \textit{Probability represents a long-run frequency over a large number of repetitions of an experiment}. 

		\textbf{The Bayesian view}: \textit{Probability represents a degree of belief about the event in question}.

		Many machine learning techniques are derived from these two views. As the computing power and algorithms develop, however, Bayesian is becoming dominant.\\

	\subsection{Transformations}

		\textbf{Change of variables}: \textit{Let }$\bm{X}=(X_1,...,X_n)$\textit{ be a continuous random vector with joint PDF }$\bm{X}$\textit{, and let }$\bm{Y}=g(\bm{X})$\textit{ where $g$ is an invertible function from $\mathbb{R}^n$ to $\mathbb{R}^n$. Then the joint PDF of $\bm{Y}$ is}
		\[f_{\bm{Y}}(\bm{y})=f_{\bm{X}}(\bm{x})\left\vert\frac{\partial\bm{x}}{\partial\bm{y}}\right\vert
		\label{eq_trans_variables}\]
		\textit{where the vertical bars say ``take the absolute value of the determinant of $\partial\bm{x}\slash \partial \bm{y}$'' and $\partial\bm{x}\slash \partial \bm{y}$ is a }\textbf{Jacobian matrix}
		\[
		\frac{\partial\bm{x}}{\partial\bm{y}} = 
		\begin{pmatrix}
		\frac{\partial x_1}{\partial y_1} & \frac{\partial x_1}{\partial y_2} & \cdots & \frac{\partial x_1}{\partial y_n} \\
		\vdots  & \vdots  & \ddots & \vdots  \\
		\frac{\partial x_n}{\partial y_1} & \frac{\partial x_n}{\partial y_2}& \cdots & \frac{\partial x_n}{\partial y_n}
		\end{pmatrix}.\]

		It assumes that the determinant of the Jacobian matrix is never 0. It also supposes all the partial derivatives $\frac{\partial x_i}{\partial y_j}$ exist and are continuous.\\

	\subsection{Limit Theorem}

		\textbf{Strong Law of Large Numbers (SLLN)}: \textit{The sample mean $\bar{X_n}$ converges to the true mean $\mu$ point-wise as $n\to \infty$, w.p.1(i.e. with probability 1). In other words, the event $\bar{X_n}\to \mu$ has probability 1.}\\

		\textbf{Weak Law of Large Numbers (WLLN)}: \textit{For all $\varepsilon >0, P(|\bar{X_n}-\mu|>\varepsilon)\to 0$ as $n\to \infty$. (This form of convergence is called} \textbf{convergence in probability} \textit{).}

		The Weak Law of Large Numbers can be proved by using \textbf{Chebyshev's inequality}.\\

		\textbf{Central Limit Theorem (CLT)}: \textit{As $n\to \infty$, $\sqrt{n}\left( \frac{\bar{X_n}-\mu}{\sigma}\right )\to\mathcal{N}(0,1)$ in distribution. In words, the CDF of the left-hand side approaches the CDF of the standard Normal distribution.}\\

	\subsection{Sampling \& Monte Carlo Methods}

		\textbf{Inverse Transform Method}: \textit{Let $F$ be a CDF which is a continuous function and strictly increasing on the support of the distribution. This ensures that the inverse function $F^{-1}$ exists, as a function from ($0,1$) to $\mathrm{R}$. We then have the following results.}

		\qquad \textit{1. Let} $U\sim\text{Unif}(0,1)$ \textit{and $X=F^{-1}(U)$. Then $X$ is an r.v. with CDF $F$.}

		\qquad \textit{2. Let $X$ be an r.v. with CDF $F$. Then} $F(X)\sim\text{Unif}(0,1)$.

		\textit{Proof}:

		1. Let $U\sim\text{Unif}(0,1)$ and $X=F^{-1}(U)$. Then we have $P(U\le u)=u$ for $u\in(0,1)$. For all real $x$,
		\[P(X\le x)=P(F^{-1}(U)\le x)=P(U\le F(x))=F(x),\]
		so the CDF of $X$ is $F$, as claimed.

		2. Let $X$ have CDF $F$, and find the CDF of $Y=F(X)$. Since $Y$ takes values in $(0,1)$, $P(Y\le y)$ equals 0 for $y\le 0$ and equals 1 for $y\ge 1$. For $y\in (0,1)$,
		\[P(Y\le y) = P(F(X)\le y)=P(X\le F^{-1}(y))=F(F^{-1}(y))=y.\]
		Thus $Y$ has the CDF of $\text{Unif}(0,1)$.
		\qed\\

		\textbf{Box-Muller}: \textit{Let }$U\sim\text{Unif}(0,2\pi)$\textit{, and let }$T\sim\text{Expo}(1)$\textit{ be independent of U. Define $X=\sqrt{2T}\cos U$ and $Y=\sqrt{2T}\sin U$. Then $X$ and $Y$ are independent and the joint PDF of $(X,Y) is$}
		\[f_{X,Y}(x,y)=\frac{1}{2\pi}e^{\frac{1}{2}(x^2+y^2)}\]

		\textit{Proof}:

		The joint PDF of $U$ and $T$ is
		\[f_{U,T}(u,t)=\frac{1}{2\pi}e^{-t},\]
		for $u\in(0,2\pi)$ and $t>0$. And we have the Jacobian matrix
		\[\frac{\partial(x,y)}{\partial(u,t)} = 
		\begin{pmatrix}
		-\sqrt{2t}\sin u & \frac{1}{\sqrt{2t}}\cos u\\
		\sqrt{2t}\cos u & \frac{1}{\sqrt{2t}} \sin u
		\end{pmatrix}.\]

		Then we have
		\[\begin{split}
		f_{X,Y}(x,y)&=f_{U,T}(u,t)\cdot \left\vert \frac{\partial(u,t)}{\partial(x,y)}\right\vert\\
		&=\frac{1}{2\pi}e^{-t}\cdot 1\\
		&=\frac{1}{2\pi}e^{-\frac{1}{2}(x^2+y^2)}\\
		&=\frac{1}{\sqrt{2\pi}}e^{-x^2\slash 2}\cdot \frac{1}{\sqrt{2\pi}}e^{-y^2\slash 2}
		\end{split}
		\]
		for all real $x$ and $y$. The joint PDF $f_{X,Y}$ factors into a function $x$ times a function of $y$, so $X$ and $Y$ are independent. Furthermore, we can find that $X$ and $Y$ are i.i.d. $\mathcal{N}(0,1)$. That shows how the \textit{Box-Muller} method works for generating Normal r.v.s.
		\qed\\

		\textbf{Monte Carlo Integration}: \textit{Given a function $\Phi:\mathbb{R}^n\to \mathbb{R}$. If $p(\cdot)$ denotes a valid PDF with the support over $\mathbb{R}^n$, then we have}
		\[
		\begin{split}
		\int_{\mathbb{R}^n}\Phi(x)dx&=\int_{\mathbb{R}^n} \frac{\Phi(x)}{p(x)}\cdot p(x)dx\\
		&=\mathbb{E}_p\left [\frac{\Phi(x)}{p(x)}\right ]\\
		&\approx \frac{1}{N}\sum_{k=1}^N \frac{\Phi(x_k)}{p(x_k)},
		\end{split}
		\label{eq_monte_carlo_int}
		\]
		\textit{where} $x_k\sim p$. 

		By the law of large numbers, the estimator converges to the true value of the integral with probability 1 as $n\to\infty$. Therefore we can use random samples to obtain approximations of definite integrals when exact integration methods are unavailable. Such approach is often referred to as the \textbf{Monte Carlo method}.\\

		\textbf{Importance Sampling}: \textit{Let $p(x)$ denote the target distribution and $\mathbb{E}_p[c(x)]$ is what we want to estimate. With a PDF $q(x)$ which subject to that $\frac{p(x)}{q(x)}$ is finite for all $x\in A$, we have}
		\[
		\begin{split}
		\mathbb{E}_p[c(x)]&=\int_A c(x)p(x)dx\\
		&=\int_A c(x)\cdot \frac{p(x)}{q(x)} \cdot q(x)dx\\
		&=\mathbb{E}_q\left[c(x)\cdot \frac{p(x)}{q(x)}\right]\\
		&\approx \frac{1}{N}\sum_{k=1}^N c(x_k)\frac{p(x_k)}{q(x_k)}
		\end{split}
		\label{eq_importance_sampling}
		\]
		\textit{where} $x_k\sim q$ \textit{and} $q$ \textit{is called} \textbf{importance distribution}.

		The estimator converges to the true value for the same reason the Monte Carlo method converges. Furthermore, the estimator with importance sampling has the less variance than that of the standard Monte Carlo method.\\

		\textbf{Acceptance-Rejection Method}: \textit{Let $X\sim p$ and $Y\sim q$ from which we can relatively easily generate samples. Then for a constant $c$ such that $c\ge \sup_\zeta \frac{p(\zeta)}{q(\zeta)}$, we can simulate $X\sim p$ with three steps:}

		\qquad \textit{Step 1: Generate $y\sim q$.}

		\qquad \textit{Step 2: Generate $u\sim$}$\text{U}(0,1)$.

		\qquad \textit{Step 3: If $u\le\frac{p(y)}{cq(y)}$, set $x=y$, otherwise go back to Step 1.}

		\textit{Proof}:

		We now show how it works.
		\[\begin{split}
		P(X\le \zeta)&=P\left(Y\le\zeta|U\le \frac{p(y)}{cq(y)}\right)\\
		&=\frac{P(Y\le \zeta, U\le \frac{p(y)}{cq(y)})}{P(U\le \frac{p(y)}{cq(y)})}\\
		&=\frac{\int_0^\zeta \int_0^{\frac{p(y)}{cq(y)}} 1du\cdot q(y)dy}{\int_{-\infty}^{+\infty}\int_0^{\frac{p(y)}{cq(y)}} 1du\cdot q(y)dy}\\
		&=\frac{\int_{-\infty}^\zeta p(y)dy}{\int_{-\infty}^{+\infty}p(y)dy}.
		\end{split}
		\]
		Given that $p$ is a valid PDF, the denominator is 1. Thus we have
		\[P(X\le\zeta)=\int_{-\infty}^\zeta p(x)dx,\]
		which shows that $X\sim p$.
		\qed\\

	\subsection{Basic Inequalities}

		\textbf{Cauchy-Schwarz Inequality}: \textit{For any r.v.s $X$ and $Y$ with finite variances,}
		\[|\mathbb{E}[XY]|\le \sqrt{\mathbb{E}[X^2]\mathbb{E}[Y^2]}.\]

		\textit{Proof}:

		For any $t$,
		\[\begin{split}
		\mathbb{E}[(Y-tX)^2]&\ge 0\\
		\mathbb{E}[(Y^2-2tXY+t^2X^2)]&\ge 0,
		\end{split}
		\]
		where the left-hand side is a quadratic function with respect to $t$. To satisfy the inequality, the discriminant of the quadratic must be less than 0, which means
		\[\begin{split}
		[2\mathbb{E}[XY]]^2-4\cdot \mathbb{E}[X^2]\mathbb{E}[Y^2]&\le 0\\
		[\mathbb{E}[XY]]^2&\le \mathbb{E}[X^2]\mathbb{E}[Y^2]\\
		|\mathbb{E}[XY]|&\le \sqrt{\mathbb{E}[X^2]\mathbb{E}[Y^2]}.
		\end{split}
		\]
		Therefore we have the Cauchy-Schwarz inequality.
		\qed\\

		\textbf{Jensen's Inequality}: \textit{Let $X$ be a random variable. If $g$ is a convex function, then $\mathbb{E}[g(X)]\ge g(\mathbb{E}[X])$. If $g$ is a concave function, then $\mathbb{E}[g(X)]\le g(\mathbb{E}[X])$. In both cases, the only way that equality can hold is if there are constants $a$ and $b$ such that $g(X)=a+bX$ with probability 1.}

		\textit{Proof}:

		If $g$ is convex, then all lines that are tangent to $g$ lie below $g$. Denoting the tangent line of $g$ by $a+bx$, we have $g(x)\ge a+bx$ for all $x$ by convexity, so $g(X)\ge a+bX$. Taking the expectation of both sides,
		\[\begin{split}
		\mathbb{E}[g(X)]&\ge \mathbb{E}[a+bX]\\
		&\ge a+b\mathbb{E}[X]\\
		&\ge g(\mathbb{E}[X]).
		\end{split}
		\]
		If $g$ is concave, then $h=-g$ is convex, so we can apply the proof to $h$ to see that the inequality for $g$ is reversed from the convex case.

		Lastly, assume that $g(X)=a+bX$ holds in the convex case. Let $Y=g(X)-a-bX$. Then $Y$ must be a nonnegative r.v. with $\mathbb{E}[Y]=0$, so $P(Y=0)=1$. So equality holds if and only if $P(g(X)=a+bX)=1$. For the concave case, the similar argument can apply to $Y=a+bX-g(X)$.
		\qed\\

		\textbf{Norm Inequality}: \textit{For a random variable $X$ whose moment of order $r>0$ is finite, we define the following norm}
		\[\Vert{X}\Vert_r=(\mathbb{E}[|X|^r])^\frac{1}{r}.\]

		With this definition, we have the following inequalities.
		\begin{itemize}[noitemsep,topsep=0pt]
			\item \textit{Holder Inequality}: \textit{Let $\frac{1}{p}+\frac{1}{q}=1$. If $\mathbb{E}[|X|^p], \mathbb{E}[|X|^q]<\infty$, then $|\mathbb{E}[XY]|\le \mathbb{E}[|XY|]\le \Vert X\Vert_p\cdot \Vert Y\Vert_q$.}
			\item \textit{Lyapunov Inequality}: \textit{For $0<r\le p$, $\Vert X\Vert_r\le\Vert X\Vert_p$.}
			\item \textit{Minkowski Inequality}: \textit{Let $p\ge 1$. If $\mathbb{E}[|X|^p], \mathbb{E}[|Y|^p]<\infty$, then $\Vert X+Y\Vert_p \le \Vert X\Vert_p+\Vert Y\Vert_p$.}\\
		\end{itemize}

		\textbf{Markov's Inequality}: \textit{For any r.v. $X$ and constant $a>0$,}
		\[P(|X|\ge a)\le \frac{\mathbb{E}[|X|]}{a}.\]

		\textit{Proof}:

		Let $Y=\frac{|X|}{a}$. We need to show that $P(Y\ge 1)\le \mathbb{E}[Y]$. Note that 
		\[I(Y\ge 1)\le Y,\]
		taking the expectation of both sides, then we have Markov's Inequality.
		\qed

		Markov's inequality is a very crude bound because it requires absolutely no assumptions about $X$. The right-hand side of the inequality could be greater than $1$ sometimes, or even infinite.\\

		\textbf{Chebyshev's Inequality}: \textit{Let $X$ have mean $\mu$ and variance $\sigma^2$. Then for any $a>0$,}
		\[P(|X-\mu|\ge a)\le \frac{\sigma^2}{a^2}.\]

		\textit{Proof}:

		By Markov's inequality,
		\[\begin{split}
		P(|X-\mu|\ge a)&=P((X-\mu)^2\ge a^2)\\
		&\le \frac{\mathbb{E}[(X-\mu)^2]}{a^2}\\
		&=\frac{\sigma^2}{a^2}.
		\end{split}
		\]
		\qed

		\textbf{Chernoff's Inequality}: \textit{For any r.v. $X$ and constants $a>0$, $t>0$, we have}
		\[P(X\ge a)\le \frac{\mathbb{E}[e^{tX}]}{e^{ta}}.\]

		\textit{Proof}:

		The transformation $g$ with $g(x)=e^{tx}$ is invertible and strictly increasing. So by Markov's inequality, we have
		\[\begin{split}
		P(X\ge a)&=P(e^{tX}\ge e^{ta})\\
		&\le \frac{\mathbb{E}[e^{tX}]}{e^{ta}}.
		\end{split}
		\]
		\qed

	\subsection{Concentration Inequalities}

		\textbf{Hoeffding Lemma}: \textit{Let $X$ be a r.v. with $\mathbb{E}[X]=0$, taking values in a bounded interval $[a, b]$, where $a$ and $b$ are constants. Then for any $\lambda>0$,}
		\[\mathbb{E}[e^{\lambda X}]\le e^{\frac{1}{8} \lambda^2(b-a)^2}.\]

		\textit{Proof}:

		For the case $a=b=0$, we have $P(X=0)=1$. The equality holds since both sides of the inequality are $1$. We now consider the general case where $a<0$ and $b>0$.

		Let $f(x)=e^{\lambda x}$ where $x\in [a,b]$. According to its convexity, for any $\alpha\in (0,1)$, we have
		\[f(\alpha a +(1-\alpha)b)\le \alpha f(a)+(1-\alpha)f(b)=\alpha e^{\lambda a}+(1-\alpha)e^{\lambda b}.\]
		As $X\in [a,b]$, let $\alpha = \frac{b-X}{b-a}$, then we have $f(\alpha a+(1-\alpha)b)=f(X)=e^{\lambda X}$. Plugging the two equations into the previous inequality, we have
		\[e^{\lambda X}\le \frac{b-X}{b-a}e^{\lambda a}+\frac{X-a}{b-a}e^{\lambda b}.
		\]
		Taking the expectation of both sides,
		\[\mathbb{E}[e^{\lambda X}]\le \frac{b}{b-a}e^{\lambda a}-\frac{a}{b-a}e^{\lambda b}=e^{\lambda a}\left[\frac{b}{b-a}-\frac{a}{b-a}e^{\lambda(b-a)}\right],\]
		and defining a function $\Phi(t)=-\theta t +\ln (1-\theta+\theta e^t)$ where $\theta=\frac{-a}{b-a} >0$, we have
		\[\mathbb{E}[e^{\lambda X}]\le e^{\Phi(\lambda(b-a))},\]
		as $e^{\Phi(\lambda(b-a))}=e^{\lambda a}\left[\frac{b}{b-a}-\frac{a}{b-a}e^{\lambda(b-a)}\right].$

		We now focus on $\Phi(t)$. According to Taylor expansion, for any $t>0$, $\exists \tau\in[0,t)$ s.t.
		\[\begin{split}
		\Phi(t)&=\Phi(0)+t\Phi'(0)+\frac{1}{2}t^2\Phi''(\tau)\\
		&=\frac{1}{2} t^2\cdot \frac{(1-\theta)\theta e^\tau}{(1-\theta+\theta e^\tau)^2}\\
		&\le \frac{1}{8}t^2
		\end{split}
		\]
		since
		\[(1-\theta+\theta e^\tau)^2=(1-\theta-\theta e^\tau)^2+4(1-\theta)\theta e^\tau\ge 4(1-\theta)\theta e^\tau.\]

		Plugging in $t=\lambda(b-a)$, we have $\Phi(\lambda(b-a))\le \frac{1}{8}\lambda^2(b-a)^2$. It follows that
		\[\mathbb{E}[e^{\lambda X}]\le e^{\frac{1}{8}\lambda^2(b-a)^2}.\]
		\qed

		\textbf{Hoeffding Bound}: \textit{Let $X_1,X_2,...,X_n$ be independent r.v.s with $\mathbb{E}[X_i]=\mu,a\le X_i\le b$ for each $i=1,2,...,n$, where $a, b$ are constants. Then for any $\varepsilon\ge 0$,}
		\[P\left(\bigg\vert \frac{1}{n}\sum_{i=1}^n X_i-\mu\bigg\vert\ge \varepsilon\right)\le 2e^{-\frac{2n\varepsilon^2}{(b-a)^2}}.\]

		\textit{Proof}:

		Let $Z_i=X_i-\mu$ and $Z=\frac{1}{n}\sum_{i=1}^n Z_i$, then we have $\mathbb{E}[Z_i]=0$ and $\mathbb{E}[Z]=0$. For any $\lambda>0$, we have
		\[P(Z\ge \varepsilon)\le \frac{\mathbb{E}[e^{\lambda Z}]}{e^{\lambda \varepsilon}}\]
		by Chernoff's inequality. For $\mathbb{E}[e^{\lambda Z}]$, we have
		\[\mathbb{E}[e^{\lambda Z}]=\mathbb{E}[e^{\lambda \frac{1}{n}\sum_{i=1}^n Z_i}]=\prod_{i=1}^n \mathbb{E}[e^{\frac{\lambda}{n}Z_i}].\]
		As $Z_i=X_i-\mu \in [a-\mu,b-\mu]$, using Hoeffding Lemma, we have
		\[\prod_{i=1}^n \mathbb{E}[e^{\frac{\lambda}{n}Z_i}]\le e^{\frac{\lambda^2}{8n}(b-a)^2}.\]
		Therefore we have
		\[P(Z\ge\varepsilon)\le e^{-\lambda\varepsilon+\frac{\lambda^2}{8n}(b-a)^2}.\]
		Now we focus on the quadratic $-\lambda\varepsilon+\frac{\lambda^2}{8n}(b-a)^2$ w.r.t $\lambda$. It is easy to find the quadratic has the minimum at $\lambda=\frac{4n\varepsilon}{(b-a)^2}$. Plugging in $\lambda=\frac{4n\varepsilon}{(b-a)^2}$, we have
		\[P(Z\ge\varepsilon)\le e^{\frac{-2n\varepsilon^2}{(b-a)^2}}.\]
		Therefore by the symmetry we have
		\[P\left(\bigg\vert \frac{1}{n}\sum_{i=1}^n X_i-\mu\bigg\vert\ge \varepsilon\right)\le2e^{\frac{-2n\varepsilon^2}{(b-a)^2}}.\]
		\qed

	\subsection{Conditional Expectation}

		\textit{``Conditional probabilities are probabilities, and all probabilities are conditional.''}

		For conditional expectation, the case is similar.\\

		\textbf{Taking out what's known}: \textit{For any function $h$,}
		\[\mathbb{E}[h(X)Y|X]=h(X)\mathbb{E}[Y|X].\]
		Intuitively, when we take expectations given $X$, we are treating $X$ as if it has crystallized into a known constant. Then any function of $X$, say h($X$), also acts like a known constant while we are conditioning on $X$.\\

		\textbf{Law of Total Expectation (LOTE)}: \textit{ Let $A_1, ..., A_n$ be a partition of a sample space, with $P(A_i ) > 0$ for all $i$, and let $Y$ be a random variable on this sample space. Then}
			\[\mathbb{E}[Y]=\sum_{i=1}^n \mathbb{E}[Y|A_i]P(A_i).\]\\

		\textbf{Adam's Law}: \textit{For any r.v.s $X$ and $Y$,}
		\[\mathbb{E}[\mathbb{E}[Y|X]] = \mathbb{E}[Y].\]

		\textit{Proof:}

		Without loss of generality, we consider the case where $X$ and $Y$ are both discrete. Let $\mathbb{E}[Y|X]=g(X)$. Expanding the definition of $g(x)$ by applying LOTUS, we have
		\begin{align*}
		\mathbb{E}[\mathbb{E}[Y|X]]&=\mathbb{E}[g(X)]\\
		&=\sum_x g(x)P(X=x)\\
		&=\sum_x \mathbb{E}[Y|X=x] P(X=x)\tag{1}\\
		&=\sum_x\left(\sum_y y P(Y=y|X=x)\right)P(X=x)\\
		&=\sum_y y \sum_x P(Y=y,X=x)\\
		&=\sum_y y P(Y=y)\\
		&=\mathbb{E}[Y]\tag{2}
		\end{align*}
		as desired. Also, as it shown in the proof, with $(1)$ and $(2)$ we can prove the LOTE.
		\qed\\

		\textbf{Adam's law with extra conditioning}: \textit{For any r.v.s $X,Y,Z$ we have}
		\[\mathbb{E}[\mathbb{E}[Y|X,Z]|Z]=\mathbb{E}[Y|Z].\]

		\textit{Proof:}

		Define the expectation $\hat{\mathbb{E}}[\cdot ] =\mathbb{E}[\cdot | Z]$. The key is that \textit{``conditional expectation is expectation''}. We have
		\begin{align*}
		\mathbb{E}[\mathbb{E}[Y|X,Z]|Z]&=\hat{\mathbb{E}}[\hat{\mathbb{E}}[Y|X]]\\
		&=\hat{\mathbb{E}}[Y] \tag{by Adam's Law}\\
		&=\mathbb{E}[Y|Z]
		\end{align*}
		\qed\\

		\textbf{Eve's law}: \textit{ For any r.v.s $X$ and $Y$,}
		\[Var(Y)=\mathbb{E}[Var(Y|X)]+Var(\mathbb{E}[Y|X]).\]
		\textit{It is also known as the law of the variance or the variance decomposition formula.}

		\textit{Proof:}
		
		Let $g(X)=\mathbb{E}[Y|X]$. By Adam's law, we have $\mathbb{E}[g(X)]=\mathbb{E}[\mathbb{E}[Y|X]]=\mathbb{E}[Y]$. According to the variance of the expectation that
		\[Var(Y|X)=\mathbb{E}[Y^2|X]-(\mathbb{E}[Y|X])^2,\]
		which can be shown by the way we prove Adam's law, we have
		\begin{align*}
		\mathbb{E}[Var(Y|X)]&=\mathbb{E}[\mathbb{E}[Y^2|X]-(g(X))^2]\\
		&=\mathbb{E}[\mathbb{E}[Y^2|X]]-\mathbb{E}[(g(X))^2]\\
		&=\mathbb{E}[Y^2]-\mathbb{E}[(g(X))^2], \tag{1}\\
		Var(\mathbb{E}[Y|X])&=\mathbb{E}[(g(X))^2]-(\mathbb{E}[\mathbb{E}[Y|X]])^2\\
		&=\mathbb{E}[g(X)^2]-(\mathbb{E}[Y])^2. \tag{2}
		\end{align*}
		Then the Eve's law can be shown by $(1)+(2)$.
		\qed\\

\pagebreak

\section{Bandit Algorithms}

	The large part of this section was done with references \cite{si252,sutton2018reinforcement,slivkins2019introduction,lattimore2018bandit,liblog}.\\

	\subsection{Bandit Models}

		As a special case of Reinforcement Learning, bandit algorithms share some important concepts of that.

		\textbf{Reward}: \textit{A reward $R_t$ is a scalar feedback signal which indicates how well agent is doing at step $t$.}

		Reinforcement learning is based on the reward hypothesis that \textit{all goals can be described by the maximization of expected cumulative reward}. Furthermore, Depends on how well we know the reward, there are three types of feedback.

		\textbf{Bandit Feedback}: \textit{the agent only knows the reward for the chosen arm;}

		\textbf{Full Feedback}: \textit{the agent knows the rewards for all arms that could have been chosen;}

		\textbf{Partial Feedback}: \textit{apart from the chosen arm, the agent knows rewards for some arms.}

		Besides the feedback, the rewards model also varies, such as \textit{IID rewards, adversarial rewards, constrained adversarial rewards, and stochastic rewards}.\\

	\subsection{Stochastic Bandits}

		The basic settings of stochastic bandits are \textit{bandit feedback} and \textit{IID reward}. Also, we assume that per-round rewards are bounded. A multi-armed bandit $<\mathcal{A,R}>$ are defined as follows:

		\begin{itemize}[noitemsep,topsep=0pt]
		\item $\mathcal{A}$: a known set of $m$ actions;
		\item $\mathcal{R}^{a}(r)=\mathbb{P}(r|a)$: an \textit{unknown} probability distribution over rewards $r$;
		\item $a_t$: the action selected by the agent at time $t$ and $a_t\in \mathcal{A}$;
		\item $r_t$: the reward generated by the environment at time $t$ and $r_t\sim \mathcal{R}^{a_t}$;
		\item $\mathbb{E}\left[\sum_{\tau=1}^t r_{\tau}\right]$: the goal we want to maximize.\\
		\end{itemize}

		For convenience, we define

		\begin{itemize}[noitemsep,topsep=0pt]
		\item $Q(a)=\mathbb{E}[r_t|a_t=a]$: the mean reward for the action $a$;
		\item $V^\ast=\max_{a\in \mathcal{A}} Q(a)$: the optimal reward for the action $a$;
		\item $L_\tau=\mathbb{E}[V^\ast-Q(a_\tau)]$: the regret for the round $\tau$, indicating the opportunity loss.\\
		\end{itemize}

		With those definitions, the goal of the multi-armed bandit $<\mathcal{A,R}>$ is equivalent to minimize

		\[L_t=\mathbb{E}\left[\sum_{\tau=1}^t(V^\ast-Q(a_\tau))\right].\label{rl_regret}\]

		That is to say, minimize the total regret we might have by not selecting the optimal action up to the time step $t$. Another way to formulate the regret is about counting:

		$N_t(a)=\sum_{\tau=1}^t I_{a_\tau=a}$: the number of selections for the action $a$ after the end of the round $t$;

		$\Delta_a=V^\ast-Q(a)$: the difference in the value between the action $a$ and the optimal action $a^\ast$;

		Then the regret follows that
		\[
		L_t=\sum_{a\in \mathcal{A}}\mathbb{E}[N_t(a)]\Delta_a,
		\label{rl_regret_count}
		\]
		which is called \textit{Regret Decomposition Lemma}. Depending on how the regret converges, we have

		\textbf{Linear regret}: \textit{The regret $L_t$ over $t$ rounds is s.t.}
		\[\lim_{t\to\infty} \frac{L_t}{t}=1.\]

		\textbf{Sublinear regret}: \textit{The regret $L_t$ over $t$ rounds is s.t.}
		\[\lim_{t\to\infty} \frac{L_t}{t}=0.\]

		\textbf{Logarithmic asymptotic regret}: \textit{The performance of any bandit algorithm is determined by similarity between optimal arm and other arms. Asymptotic total regret is at least logarithmic in number of steps}:
		\[\lim_{t\to\infty}L_t\ge \log t\sum_{a|\Delta_a>0}\frac{\Delta_a}{KL(\mathcal{R^a}||\mathcal{R^\ast})}.\]
		The lower bound is also known as \textit{Lai-Robbins} lower bound. Such lower bound can be obtained with advance knowledge of the gap.\\

		\textbf{The Exploration-Exploitation Dilemma}: \textit{As the action-values are unknown, we must both try actions to learn the action-values (explore), and prefer those that appear best (exploit).}\\

	\subsection{Greedy Algorithms}

		Greedy algorithm tends to take the best action most of the time, while doing exploration occasionally.

		\textbf{Greedy Action}: \textit{Define the greedy action at time $t$ as
		\[a^\varepsilon_t=\arg\max_{a\in \mathcal{A}} Q(a).\]
		If $a_t=a_t^\varepsilon$, we are making exploitation, otherwise, we are making exploration, i.e. selecting an arm arbitrarily.}

	 	Usually $Q(a)$ is estimated by Monte-Carlo evaluation:
	 	\[\hat{Q}_t(a)=\frac{1}{N_{t-1}(a)}\sum_{\tau=1}^{t-1}r_\tau I(a_\tau=a).\]

	 	\textbf{$\varepsilon$-greedy algorithm}: \textit{Rather than making exploitation every round, we make exploitation with probability $1-\varepsilon$, or make exploration with probability $\varepsilon$, which usually is a small number.}\\

	 	\begin{algorithm}[H]
		\caption{$\varepsilon-$greedy}
		\label{alg: eps}
		\begin{algorithmic}[1]
			\State initialize $\hat{Q}(a)\gets 0,\ N(a)\gets0,\ \forall a\in\mathcal{A}$;
			\For {each time slot}:
				\State $a'\gets\begin{cases}\arg\max_{a} \hat{Q}(a) & \text{with probability }1-\varepsilon;\\\text{a random action} & \text{with probability }\varepsilon;\end{cases}$
				\State $r\gets bandit(a')$;
				\State $N(a')\gets N(a')+1$;
				\State $\hat{Q}(a')\gets \hat{Q}(a')+\frac{1}{N(a')}(r-\hat{Q}(a'))$;
        	\EndFor
		\end{algorithmic}
		\end{algorithm}

		In practice, it is useful to initialize $\hat{Q}(a)$ with high values. Such trick is called \textit{optimistic initialization}.\\

        \textbf{Decaying $\varepsilon_t$-greedy algorithm}: \textit{The greedy degree $\varepsilon$ in decaying version is not a constant. Rather, we have a decay schedule for $\varepsilon_t$ as}
        \[\varepsilon_t=\min\left\{1,\frac{c|\mathcal{A}|}{d^2t}\right\},\]
        \textit{where $c>0$ and $d=\min_{a|\Delta a>0}\Delta_i$.}

        For decaying $\varepsilon$-greedy algorithm, it usually has a better performance than the vanilla version, however, the schedule for $\varepsilon_t$ requires advance knowledge of gaps $\Delta_a$.\\

    \subsection{UCB Algorithms}

		In $\varepsilon$-greedy algorithm, we make exploration purely randomly, which may waste the opportunity to try out other options. To avoid such inefficient exploration, UCB algorithm allows us to pick the arm with the highest upper bound with high probability. 

    	\textbf{UCB algorithm}: \textit{Estimate an upper confidence bound $\hat{U}_t(a)$ for each $Q(a)$, i.e.}
    	\[Q(a)\le \hat{Q}_t(a)+\hat{U}_t(a).\]
    	\textit{Then select actions with max upper confidence bound, i.e.}
    	\[a_t=\arg\max_{a\in \mathcal{A}}\{\hat{Q}_t(a)+\hat{U}_t(a)\}.\]

    	In other words, UCB algorithm favors exploration of actions with a strong potential to have an optimal value. When we don't have any prior knowledge, the bound can be determined by \textit{Hoeffding's Inequality}. It follows that
    	\[P(Q(a)> \hat{Q}_t(a)+\hat{U}_t(a))\le e^{-2N_t(a)U_t^2(a)}.\]
    	Since we want to pick a bound so that with high chances the true mean $Q(a)$ is blow the sample mean $\hat{Q}_t(a)$ $+$ the upper confidence bound $\hat{U}_t(a)$, the right-hand side of the inequality should be a small probability, for example, a tiny threshold $p$, therefore solving for $\hat{U}_t(a)$ we have
    	\[\hat{U}_t(a)=\sqrt{\frac{-\log p}{2N_t(a)}}.\]
    	The upper bound $\hat{U}_t(a)$ is a function of $N_t(a)$ which means a larger number of trials $N_t(a)$ should give us a smaller bound $\hat{U}_t(a)$. Further, we want the small bound to have a high probability, which we can achieve by designing the $p$.

    	\textbf{UCB1}: \textit{As we want to make more confident bound estimation with more rewards observed, the threshold $p$ should be reduced in time. A reasonable idea is setting $p=t^{-4}$ and then we get UCB1 algorithm}
    	\[\hat{U}_t(a)=\sqrt{\frac{2\log t}{N_t(a)}},\]
    	\[a_t^{UCB1}=\arg\max_{a\in \mathcal{A}}\left\{\hat{Q}(a)+\sqrt{\frac{2\log t}{N_t(a)}}\right\}.\]
    	The regret bound of UCB1 is
    	\[\lim_{t\to\infty}L_t\le 8\log t\sum_{a|\Delta_a>0} \Delta_a.\]

    	\begin{algorithm}[H]
		\caption{UCB1}
		\label{alg: ucb}
		\begin{algorithmic}[1]
			\State initialize $\hat{Q}(a)\gets 0,\ N(a)\gets0,\ \forall a\in\mathcal{A}$;
			\For {each $a\in\mathcal{A}$}:
				\State $\hat{Q}(a)\gets bandit(a)$;
				\State $N(a)\gets 1$;
			\EndFor
			\For {each time slot}:
				\State $a'\gets\arg\max_{a} \left(\hat{Q}(a)+c\cdot\sqrt{\frac{2\log t}{N(a)}}\right)$;
				\State $r\gets bandit(a')$;
				\State $N(a')\gets N(a')+1$;
				\State $\hat{Q}(a')\gets \hat{Q}(a')+\frac{1}{N(a')}(r-\hat{Q}(a'))$;
        	\EndFor
		\end{algorithmic}
		\end{algorithm}

    	An intuitive explanation of why UCB works better than greedy algorithm lies in how it handle exploitation \textit{v.s} exploration: when $\hat{Q}_t(a)$ is large, specifically, $\hat{Q}_t(a)\gg \hat{U}_t(a)$, it indicates a high expected reward and leads to exploitation; when $N_t(a)$ is small, concretely, $\hat{Q}_t(a)\ll \hat{U}_t(a)$, it indicates the action may have a great potential and leads to exploration.\\

    	So far we have made no assumptions about the reward distribution $\mathcal{R}$, and therefore we have to rely on the Hoeffding's Inequality for a very generalize estimation. If we are able to know the distribution upfront, we would be able to make better bound estimation.

    \subsection{Thompson Sampling Algorithms}

    	Thompson Sampling algorithm makes use of the posterior to guide exploration. To be specific, at each time step, we want to select action a according to the probability that $a$ is the optimal action:
    	\[\pi(a|h_t)=P(Q(a)>Q(a'),\ \forall a'\ne a|h_t),\]
    	where $h_t=a_1,r_1,...,a_{t-1},r_{t-1}$ is the history.

    	\textbf{Thompson sampling}: \textit{Thompson sampling use Bayes' law to compute posterior distribution $P(\mathcal{R}|h_t)$ and compute the action-value function $\hat{Q}(a)=f(\mathcal{R}_a)$, then it selects the action}
    	\[a_t^{TS}=\arg\max_{a\in \mathcal{A}} \hat{Q}(a).\]

    	A naive example for Thompson sampling is for Beta-Bernoulli Bandit. It assumes that the action $k$ being played produces a reward satisfying \textit{Bern}$(\theta_k)$ and $\theta_k$ follows a \textit{Beta}$(\alpha_k,\beta_k)$, where $\theta_k$ is known but we have $\alpha_k,\beta_k$. Let $a_t$ denote the action selected at time $t$ and $r_t\in\{0,1\}$ denote the corresponding result of action $a_t$. Then we can update the parameters of the \textit{Beta} distribution
    	\[(\alpha_k,\beta_k)\gets\begin{cases}(\alpha_k,\beta_k) &\text{if } a_t\ne k,\\ (\alpha_k,\beta_k)+(r_t,1-r_t) &\text{if } a_t=k.\end{cases}\]

    	\begin{algorithm}[H]
		\caption{Thompson Sampling}
		\label{alg: ts}
		\begin{algorithmic}[1]
			\State initialize $\alpha_a = |\mathcal{A}|,\beta_a=|\mathcal{A}|, N(a)\gets0,\ \forall a\in\mathcal{A}$;
			\For {each time slot}:
			\For {each $a\in\mathcal{A}$}:
				\State Sample $\hat{Q}(a)$ from $\text{Beta}(\alpha_a,\beta_a)$;
				\State $N(a)\gets1$;
			\EndFor
				\State $a'\gets\arg\max_{a} \left(\hat{Q}(a)+c\cdot\sqrt{\frac{2\log t}{N(a)}}\right)$;
				\State $r\gets bandit(a')$;
				\State $N(a')\gets N(a')+1$;
				\State $\hat{Q}(a')\gets \hat{Q}(a')+\frac{1}{N(a')}(r-\hat{Q}(a'))$;
        	\EndFor
		\end{algorithmic}
		\end{algorithm}

    	For the details of how it works one can refer to the idea of \textit{Beta Bernoulli Conjugate}. It exploits the power of Bayesian inference to compute the posterior and finally achieves a probability matching. However, for many practical and complex problems, it can be computationally intractable to estimate the posterior distributions with observed true rewards using Bayesian inference. In that case, we may need to approximate the posterior distributions using methods like Gibbs sampling and Laplace approximate.\\

    \subsection{Gradient Bandit Algorithms}

    	Previously our bandit algorithms are based on the estimation of action values. while the gradient bandit algorithm is based on a new criterion.

    	\textbf{Gradient Bandit Algorithm}: \textit{Let $H_t(a)$ be a learned preference for taking action $a$, then we have the rules}
    	\[P(A_t=a)=\frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}}=\pi_t(a),\]
    	\textit{and}
    	\[H_{t+1}(a)=H_t(a)+\alpha(r_t-\bar{r_t})[I(A_t=a)-\pi_t(a)],\]
    	\textit{where }$\bar{r_t}=\frac{1}{t}\sum_{i=1}^t r_i$ \textit{ and the learning rate $\alpha>0$}.

    	The algorithm is also known as \textit{stochastic gradient ascent algorithm} since the term $(r_t-\bar{r_t})[I(A_t=a)-\pi_t(a)]$ is essentially the stochastic gradient of $\mathbb{E}[r_t]$ with respect to $H_t(a)$.

    	\begin{algorithm}[H]
		\caption{Gradient Bandit}
		\label{alg: gb}
		\begin{algorithmic}[1]
			\State initialize $R=0, H(a)=0,\ \forall a\in\mathcal{A}$;
			\For {each time slot $t$}:
			\For {each $a\in\mathcal{A}$}:
				\State $\pi(a) = \frac{H(a)}{\sum_{a'} H(a')}$;
			\EndFor
			\State Sample action $a'$ from $\pi$;
			\State $r\gets bandit(a')$;
			\For {each $a\in\mathcal{A}$}:
				\State $H(a) = H(a) + \alpha(r-R)[I(a'=a)-\pi(a)]$;
				\State $R\gets \frac{1}{t}[(t-1)R+r]$;
        	\EndFor
        	\EndFor
		\end{algorithmic}
		\end{algorithm}

    	Actually, gradient bandit algorithm has a bad performance when compared with other algorithms. However, the introduction of the gradient and the function inspires many policy gradient based algorithms in Reinforcement Learning.

    	\pagebreak

\pagebreak

\section{Markov Chains}

	The large part of this section was done with references \cite{si252,HB,robert2013monte}.\\

	\subsection{Markov Model}

		\textbf{Stochastic Processes}: \textit{A stochastic process is a collection, or, a sequence of random variables $\{X_t,t\in \mathcal{T}\}$. The set $\mathcal{T}$ is the index set of the process. All the \textit{r.v.}s are defined on a common state space $\mathcal{S}$}.\\
		
		\textbf{Markov Property}: \textit{Given a stochastic process $X_0, X_1, X_2, ..., X_n$ taking values in the state space $\mathcal{S}$, the future evolution of the process is independent of the past evolution of the process, i.e.,}
		\[P(X_{n+1}=j|X_n=i_n,X_{n-1}=i_{n-1},...,X_0=i_0)=P(X_{n+1}=j|X_n=i_n).\]

		The above equation holds for the first order Markov property. For the second order Markov property we have $P(X_{n+1}|X_n, ...,X_0)=P(X_{n+1}|X_n,X_{n-1})$, \textit{etc}. With Markov property, many calculation tasks can be simplified greatly.\\
	
		\textbf{Markov Chain/Process}: \textit{A sequence of random variables $X_0, X_1, X_2, ...$ taking values in the state space $\mathcal{S}$ is called a Markov chain if it has Markov property. A Markov process is the continuous-time version of a Markov Chain.}\\

		\textbf{Transition Matrix}: \textit{For a Markov Chain, let $q_{ij}=P(X_{n+1}=j|X_n=i)$ be the transition probability from state $i$ to state $j$. Then the matrix $Q$ is called the transition matrix of the chain.}

		Transition Matrix is a common way to express a Markov chain. Besides that, Markov chain can be represented in a graphical form.\\

	\subsection{Basic Computations}

		With a little abuse of notation, I would use $Q$ to denote the transition matrix when we talk about Markov chain, and $q^n_{i,j}$ to denote the entry $(Q^n)_{i,j}$. Now we introduce some useful computations.\\

		\textbf{$n$-step Transition Probability}: \textit{For a Markov chain, the $n$-step transition probability from $i$ to $j$ is the probability of being at $j$ exactly $n$ steps after being at $i$, and}
		\[P(X_{n+m}=j|X_m=i)=q^n_{i,j}.\]

		\textit{Proof}:

		For a Markov chain, the states are time-homogeneous. Thus we have 
		\[P(X_{n+m}=j|X_m=i)=P(X_n=j|X_o=i).\]
		Hence it follows that
		\begin{align*}
		P(X_n=j|X_0=i)&=\sum_k P(X_n=j, X_{n-1}=k | X_0=i )\tag{by LOTP}\\
		&=\sum_k P(X_n=j|X_{n-1}=k, X_0=i)P(X_{n-1}=k|X_0=i)\\
		&=\sum_k q_{k,j}P(X_{n-1}=k|X_0=i),\tag{by Markov Property}
		\end{align*}
		then by \textit{induction} from 2 to $n-1$, we have
		\[P(X_{n+m}=j|X_m=i)=q^n_{i,j}.\]
		\qed

		With $n$-step transition probability, we have the \textit{Chapman-Kolmogorov Equation}.

		\textbf{Chapman-Kolmogorov Equation}: \textit{For $m,n\ge 0$, we have}
		\[P(X_{m+n}=j|X_0=i)=\sum_k P(X_m=k|X_0=i)P(X_n=j|X_0=k).\]

		The equation can be proved by matrix identity that $q^{m+n}_{i,j}=\sum_{k}q^m_{i,k}q^n_{k,j}$. By the equation, for a Markov chain with transition matrix $P$, the Markov property can be generalized to
		\[P(X_{n+1}=j|X_{n-m}=i, X_{n-m-1}=i_{n-m-1}, ..., X_0=i_0)=P(X_{n+1}=j|X_{n-m}=i)=q^{m+1}_{i,j}\]
		for $m<n$ and $m\ge0$.\\

	\subsection{Classifications}

		Depending on whether they are visited over and over again in the long run or are eventually abandoned, the states of a Markov chain can be classified as recurrent or transient.

		\textbf{Recurrent and Transient states}: \textit{State $i$ of a Markov chain is recurrent if starting from $i$, the chain can always return to $i$. Otherwise, the state is transient, which means that if the chain starts from $i$, there is a positive probability of never returning to $i$.}\\

		\textbf{Irreducible and Reducible Chain}: \textit{A Markov chain with transition matrix $Q$ is irreducible if for any two sates $i$ and $j$, it is possible to go from $i$ to $j$ in a finite number of steps (with positive probability). That is, for any states $i,j$ there is some positive integer $n$ such that the $(i,j)$ entry of $Q^{n}$ is positive. A Markov chain that is not irreducible is called reducible.}

		In an irreducible Markov chain with a finite state space, all states are recurrent.\\

		\textbf{Period}: \textit{For a Markov chain with transition matrix $Q$, the period of state $i$, denoted $d(i)$, is the greatest common divisor of the set of possible return times to $i$. That is,}
		\[d(i)=\text{gcd}\{n>0\ |\ q_{i,i}^{n}>0\}.\]
		\textit{If $d(i)=1$, state $i$ is said to be aperiodic. If the set of return times is empty, set $d(i)=+\infty$.}\\

	\subsection{Stationary Distribution}

		\textbf{Stationary Distribution}: \textit{A row vector $\mathbf{s}=(s_1,...,s_M)$ such that $\sum_i s_i=1$ is a stationary distribution for a Markov chain with transition matrix $Q$ if}
		\[\sum_i s_i q_{i,j}=s_j\]
		\textit{for all $j$.}

		Any irreducible Markov chain has a unique stationary distribution.\\

		\textbf{Doubly Stochastic Matrix}: \textit{A nonnegative matrix such that the row sums and the column sums are all equal to 1 is called a doubly stochastic matrix.}

		If the transition matrix $Q$ of a Markov chain is a doubly stochastic matrix, then the uniform distribution over all states, $(1/M,1/M,...,1/M), M=|\mathcal{S}|$, is a stationary distribution of the chain.\\

		\textbf{Convergence to Stationary Distribution}: \textit{Let $X_0, X_1, ...$ be a Markov chain with stationary distribution $\mathbf{s}$ and transition matrix $Q$, such that some power $Q^m$ is positive in all entries. (These assumptions are equivalent to assuming that the chain is irreducible and aperiodic.) Then $P(X_n = i)$ converges to $s_i$ as $n\to \infty$. In terms of the transition matrix, $Q^n$ converges to a matrix in which each row is $\mathbf{s}$.}\\

		\textbf{Ergodic Markov chain}: \textit{A Markov chain is called ergodic if it is irreducible, aperiodic, and all states have finite expected return times (positive recurrent).}

		For an ergodic Markov chain $X_0,X_1,...$, there exists a unique stationary distribution $\pi$, which is the limiting distribution of the chain. That is
		\[\pi_j=\lim_{n\to\infty} q_{i,j}^n,\ \forall i,j.\]\
		
	\subsection{Reversibility}

		\textbf{Reversibility}: \textit{Let $Q$ be the transition matrix of a Markov chain. Suppose there is $\mathbf{s} = (s_1,...,s_M)$ with $s_i\ge0,\sum_i s_i=1$, such that}
		\[s_iq_{i,j}=s_jq_{j,i}\]
		\textit{for all pairs of states $i$ and $j$.}

		This equation is called the reversibility or detailed balance condition. We say that the chain is reversible with respect to $\mathbf{s}$ if it holds, and such $\mathbf{s}$ is a stationary distribution of the chain.\\

		\textbf{Detailed Balance Equation}: \textit{If for an irreducible Markov chain with transition matrix $Q$, there exists a probability solution $\pi$ to the detailed balance equations}
		\[\pi_i q_{i,j}=\pi_j q_{j,i}\]
		\textit{for all pairs of states $i$ and $j$, then this Markov chain is positive recurrent, time-reversible and the solution $\pi$ is the unique stationary distribution.}\\

	\subsection{Markov Chain Monte Carlo}

		\textit{Monte Carlo} method is a simulation approach where we generate random values to approximate a quantity. A basic form of such method is directly generating \textit{i.i.d.} draws $X_1,X_2,..., X_n$ from a given distribution, then by the law of large numbers we can make a desired approximate if $n$ is large. However, staring at a density function does not immediately suggest how to get a random variable with that density.

		Fortunately, for this limitation, we have \textit{Markov chain Monte Carlo} (MCMC), a powerful collection of algorithms, to enable us to simulate from complicated distributions using Markov chains. The basic idea is to \textit{build your own Markov chain} so that the distribution of interest is the stationary distribution of the chain.\\

		\textbf{Convergence to stationary distribution}: \textit{Let $X_0, X_1,...$ be a Markov chain with stationary distribution $s$ and transition matrix $Q$, such that the chain is irreducible and aperiodic. Then $P(X_n=i)$ converges to $s_i$ as $n\to\infty$.}

		With the above theorem, which actually has been mentioned in \textit{ergodic Markov chain}, we can approach the desired $s$ by running our chain for a long time.\\

		\textbf{Metropolis-Hastings}: Metropolis-Hastings allows us to start with any \textit{irreducible} Markov chain on the state space of interest and then modify it into a new Markov chain that has the desired stationary distribution. 

		\textit{Recall}: In an irreducible Markov chain, for any two states $i$ and $j$ it is possible to go from $i$ to $j$ in a finite number of steps.

		\textbf{Metropolis-Hastings Algorithm}: \textit{Let $s=(s_1,...,s_M)$ be a desired stationary distribution on state space. Suppose that $Q=q_{ij}$ is the transition matrix for any irreducible Markov chain on state space $\{1,...,M\}$. Then we can use a chain with transition matrix $Q$ to construct a collection of states sample $X_0,X_1,...$ with stationary distribution $s$.}

    	\begin{algorithm}[H]
		\caption{Metropolis-Hastings}
		\label{alg: Metropolis-Hastings}
		\begin{algorithmic}[1]
			\State \textbf{input} the desired distribution $s=(s_1,...,s_M)$; the chain with transition matrix $Q$; the initial state $X_0$;
			\For {$n=0,1,...$}: \textit{ \# assume that $X_n=i$};
				\State {Sample the next state $j$ according to $Q$};
				\State {Calculate the acceptance probability $a_{ij}=\min \left(\frac{s_jq_{ji}}{s_iq_{ij}}, 1\right)$};
				\State $X_{n+1}\gets\begin{cases}j & \text{with probability }a_{ij};\\i & \text{with probability }1-a_{ij};\end{cases}$
        	\EndFor
		\end{algorithmic}
		\end{algorithm}

		In practice, a useful trick is \textit{Burn-in}, which discards the initial iterations and retains $X_m,X_{m+1},...$ for some $m$. The key of the algorithm is that the moves are proposed according to the original chain, but the proposal may or may not be accepted. By the \textit{reversibility condition}, it can be showed that the sequence $X_0,X_1,...$ constructed by the Metropolis-Hastings algorithm is a \textit{reversible} Markov chain with stationary distribution $s$.\\

		\textbf{Gibbs Sampler}: Gibbs sampling is an MCMC algorithm for obtaining approximate draws from a joint distribution, based on sampling from conditional distributions one at a time: at each stage, one variable is updated (keeping all the other variables fixed) by drawing from the conditional distribution of that variable given all the other variables.

		\textbf{Gibbs Sampling Algorithm}: \textit{Let $X$ and $Y$ be discrete r.v.s with joint PMF $p_{X,Y} (x,y) = P(X = x,Y = y)$. We wish to construct a two-dimensional Markov chain $(X_n, Y_n)$ whose stationary distribution is $p_{X,Y}$. The systematic scan Gibbs sampler proceeds by updating the $X$-component and the $Y$-component in alternation. If the current state is $(X_n,Y_n) = (x_n,y_n)$, then we update the $X$-component while holding the $Y$-component fixed, and then update the $Y$ -component while holding the $X$-component fixed.}

		\begin{algorithm}[H]
		\caption{Gibbs Sampling}
		\label{alg: Gibbs Sampling}
		\begin{algorithmic}[1]
			\State \textbf{input} the desired joint distribution $P(X,Y)$; the initial state $X_0, Y_0$;
			\For {$n=0,1,...$}:
				\State Sample the next state $X_{n+1}$ from $P(X, Y=Y_n)$;
            	\State Sample the next state $Y_{n+1}$ from $P(X=X_{n+1}, Y)$;
        	\EndFor
		\end{algorithmic}
		\end{algorithm}

        The algorithm can be generalized to high dimensional easily in the light of line 3 and 4.

\pagebreak

\section{Markov Decision Process}

	The large part of this section was done with references \cite{si252,introRL,ucl_rl,sutton2018reinforcement}.\\

	\subsection{Markov Reward Process}

		\textbf{Markov Reward Process} (MRP): \textit{A Markov Reward Process is a tuple $\langle \mathcal{S,P}, R,\gamma\rangle$ where}
		\begin{itemize}[noitemsep,topsep=0pt]
			\item$\mathcal{S}$: the (finite) set of states;
			\item$\mathcal{P}$: the state transition probability matrix, where $\mathcal{P}_{ss'}=P(S_{t+1}=s'|S_t=s)$ is the probability of arriving $s'$ from $s$;
			\item$R$: the random variable or function of rewards, and we use $R_t$ to represent the reward at time $t$;
			\item$\gamma:\ $ the discount factor.
		\end{itemize}

		An MRP chain is a Markov chain with values, and we can define \textit{return} as follows.\\

		\textbf{Return}: \textit{Suppose the reward at time $t$ is $R_t$, then the return $G_t$ is the total discounted reward from time-step $t$:}
		\[G_t=R_{t+1}+\gamma R_{t+2}+...=\sum_{k=0}^\infty \gamma^kR_{t+k+1},\]
		\textit{where $\gamma\in(0,1)$ is the discount factor.}

		There are many reasons to consider discounted rewards. One reason is that uncertainty about the future may not be fully represented. And for the case that the reward is financial, immediate rewards may earn more interest than delayed rewards. Besides, animal and human behavior shows preference for immediate reward.\\

		\textbf{Value Function}: \textit{The state value function $v(s)$ of an MRP is the expected return starting from state $s$}
		\[v(s)=\mathbb{E}[G_t|S_t=s].\]\

		\textbf{Bellman Equation for MRPs}: \textit{The value function $v(s_t)$ can be decomposed into two parts: the immediate reward $R_{t+1}$ and the discounted value of the successor state, which is}
		\[v(s)=\mathbb{E}[R_{t+1}+\gamma v(S_{t+1})|S_t=s].\]

		\textit{Proof:} 

		Notice that $S$ is a random variable of states, and $s$ is an instance of states. According to the Adam's Law with extra conditioning, it follows that 
		\[\mathbb{E}[\mathbb{E}[G_{t+1}|S_{t+1},S_t]|S_t]=\mathbb{E}[G_{t+1}|S_t].\tag{1}\]

		By Markov property, the term $(G_{t+1}|S_{t+1},S_t)$ equals $(G_{t+1}|S_{t+1})$. Therefore we arrive at
		\begin{align*}
		\mathbb{E}[\mathbb{E}[G_{t+1}|S_{t+1},S_t]|S_t]&=\mathbb{E}[\mathbb{E}[G_{t+1}|S_{t+1}]|S_t]\\
		&=\mathbb{E}[v(S_{t+1})|S_t].\tag{2}
		\end{align*}

		With (1) and (2), we can make a conclusion that
		\[\mathbb{E}[G_{t+1}|S_t]=\mathbb{E}[v(S_{t+1})|S_t].\tag{3}\]

		Then we have
		\begin{align*}
		v(s) &=\mathbb{E}[G_t|S_t=s]\\
		&=\mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t=s]\\
		&=\mathbb{E}[R_{t+1}|S_t=s]+\gamma \mathbb{E}[G_{t+1}|S_t=s]\\
		&=\mathbb{E}[R_{t+1}|S_t=s]+\gamma \mathbb{E}[v(S_{t+1})|S_t=s]\tag{by (3)}\\
		&=\mathbb{E}[R_{t+1}+\gamma v(S_{t+1})|S_t=s]
		\end{align*}
		\qed

		We can also derive another form of Bellman equation by the law of total expectation:
		\begin{align*}
		v(s) &=\mathbb{E}[R_{t+1}|S_t=s]+\gamma \mathbb{E}[v(S_{t+1})|S_t=s]\\
		&=R_s+\gamma\sum_{s'\in S}\mathbb{E}[v(S_{t+1})|S_t=s, S_{t+1}=s']\mathcal{P}_{ss'}\tag{by LOTE}\\
		&=R_s + \gamma \sum_{s'\in S} v(s')\mathcal{P}_{ss'},
		\end{align*}
		Though Bellman equation is elegant, to solve it directly is only possible for small MRPs.\\

	\subsection{Markov Decision Process}

		\textbf{Markov Decision Process} (MDP): \textit{A Markov Decision Process is a tuple $\langle \mathcal{S, A, P}, R, \gamma\rangle$, where}
		\begin{itemize}[noitemsep,topsep=0pt]
			\item$\mathcal{S}$: the (finite) set of states;
			\item$\mathcal{A}$: the (finite) set of actions;
			\item$\mathcal{P}$: the state transition probability matrix, where $\mathcal{P}_{ss'}^a=P(S_{t+1}=s'|S_t=s, A_t=a)$ is the probability of arriving $s'$ by taking action $a$ from $s$;
			\item$R$: the random variable or function of rewards, besides the notation we defined in MRP, we also use $R_s^a$ to denote the reward we get by taking action $a$ at state $s$;
			\item$\gamma:\ $ the discount factor.
		\end{itemize}

		An MDP chain is an MRP chain with decisions (actions). It is an environment in which all states are Markovian.\\

		\textbf{Policy}: \textit{A policy $\pi$ is a distribution over actions given states,}
		\[\pi(a|s)=P(A_t=a|S_t=s).\]

		A policy can fully define the behavior of an agent, and it depends only on the current state. Similar to MRP, we have the following definitions.\\

		\textbf{State Value Function}: \textit{The state value function $v^\pi(s)$ of an MDP is the expected return starting from state $s$, and following policy $\pi$}
		\[v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s].\]\

		\textbf{State-action Value Function}: \textit{The state-action value function $q^\pi(s,a)$ is the expected return starting from state $s$, taking action $a$, and then following policy $\pi$, which is}
		\[q^\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s, A_t=a].\]\

		According to the definition, we have
		\[v_\pi(s)=\sum_{a\in \mathcal{A}}\pi(a|s)q^\pi(s,a),\]
		\[q^\pi(s,a)=R_s^a+\gamma\sum_{s'\in \mathcal{S}}\mathcal{P}_{ss'}^av^\pi(s').\]
		Then the relation between $v^\pi(s)$ and $q^\pi(s,a)$ follows that
		\[v^{\pi}(s) =\sum_{a \in \mathcal{A}} \pi(a | s)\left( R_s^a+\gamma \sum_{s^{\prime} \in S} \mathcal{P}_{ss'}^a v^{\pi}\left(s^{\prime}\right)\right),\]
		\[q^{\pi}(s, a) =R_s^a+\gamma \sum_{s^{\prime} \in S} \mathcal{P}_{ss'}^a \sum_{a^{\prime} \in \mathcal{A}} \pi\left(a^{\prime} | s^{\prime}\right) q^{\pi}\left(s^{\prime}, a^{\prime}\right).\]
		And we can rewrite the above equations to get \textit{Bellman expectation equation}.\\

		\textbf{Bellman Expectation Equation}: \textit{The value function can be decomposed into immediate reward plus discounted value of the successor state, that is to say,}
		\[\begin{split}
		v^\pi(s)&=\mathbb{E}_\pi[R_{t+1}+\gamma v^\pi(S_{t+1})|S_t=s],\\
		q^\pi(s,a)&=\mathbb{E}_\pi[R_{t+1}+\gamma q^\pi(S_{t+1}, A_{t+1})|S_t=t,A_t=a].
		\end{split}
		\]\

		\textbf{Optimal State Value Function}: \textit{The optimal state value function $v_\ast(s)$ is the maximum value function over all polices:}
		\[v^\ast(s)=\max_\pi v^\pi(s).\]\

		\textbf{Optimal State-action Value Function}: \textit{The optimal state-action value function $q_\ast(s,a)$ is the maximum value function over all polices:}
		\[q^\ast(s,a)=\max_\pi q^\pi(s,a).\]\

		\textbf{Optimal Policy}: \textit{For any Markov Decision Process, there exists an optimal policy $\pi_\ast$ that is better than or equal to all other policies. Further, all optimal policies achieve the optimal value function, $v^{\pi_\ast}(s)=v_\ast(s)$ and the optimal state-action value function, $q^{\pi_\ast}(s,a)=q_\ast(s,a)$.}

		We say that an MDP is \textit{solved} when we know the optimal value function. Since the optimal policy can be derived from the optimal value function.\\

		\textbf{Prediction}: \textit{Given an MDP and a policy $\pi$, find the value function $v^\pi$ and $q^\pi$}.\\

		\textbf{Control}: \textit{Given an MDP, find the optimal policy $\pi^\ast$}.\\

		Actually, the term \textit{prediction} can be replaced with \textit{evaluation} in many cases. Hope it would not make you confused. For an unknown MDP, we have no idea about its transition matrix and its state, action spaces, thus the methods we mentioned in previous sections fail to work. What we can do is interacting with the environment and collecting episodes.\\

	\subsection{Dynamic Programming}

		Dynamic programming is a method for solving complex problems by breaking them down into subproblems. Specifically, the problems should be characterized by the two properties:
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Optimal substructure, which allows that optimal solution can be decomposed into subproblems;
			\item Overlapping subproblems, which entails that the solutions for subproblems can be cached and reused.
		\end{itemize}

		Obviously, MDPs satisfy both properties. Evaluation and control in MDP can be solved by dynamic programming.\\

	\subsubsection{Policy Evaluation}

		\textbf{Bellman expectation backup}: \textit{Given a policy $\pi$, at each iteration $k+1$, for all states $s\in \mathcal{S}$, update $v_{k+1}(s)$ from $v_k(s')$ by}
		\[v_{k+1}(s)=\sum_{a\in\mathcal{A}}\pi(a|s)\left(R_s^a+\gamma\sum_{s'\in\mathcal{S}}\mathcal{P}_{ss'}^a v_k(s')\right).\]

		Clearly, $v_k = v^\pi$ is a fixed point for this update rule because the Bellman equation for $v^\pi$ assures us of equality in this case. Indeed, the sequence $\{v_k\}$ can be shown in general to converge to $v^\pi$ as $k\to\infty$ under the same conditions that guarantee the existence of $v^\pi$. This algorithm is called iterative policy evaluation.\\

		\begin{algorithm}[H]
		\caption{Iterative Policy Evaluation}
		\label{alg: Iterative Policy Evaluation}
		\begin{algorithmic}[1]
			\State initialize $v(s)$ arbitrarily for $s\in\mathcal{S}$ except that $v(terminal)=0$;
			\State \textbf{input} the policy $\pi$ to be evaluated; the threshold $\theta>0$ determining the accuracy of the evaluation; 

			\For {true}:
				\State $\Delta\gets 0$;
				\For {each $s\in\mathcal{S}$}:
					\State $old\gets v(s)$;
					\State $v(s)\gets\sum_{a}\pi(a|s)\sum_{s'}\mathcal{P}_{ss'}^a[R_s^a+\gamma v(s')]$;
					\State $\Delta\gets\max(\Delta,|old-v(s)|)$;
				\EndFor
				\If {$\Delta<\theta$}:
					\State break;
				\EndIf
        	\EndFor
		\end{algorithmic}
		\end{algorithm}

        Formally, iterative policy evaluation converges only in the limit, but in practice it is better to be halted short of this.\\

	\subsubsection{Policy Iteration}
		\label{sec: PI}

		For a deterministic policy $a =\pi(s)$, we can iteratively improve it through the two steps:
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Evaluate the policy $\pi$ so that get the value function;
			\item Improve the policy by acting greedily for all $s\in\mathcal{S}$ with respect to the value function:
			\[\pi'(s)=\arg\max_{a}q^\pi(s,a).\]
		\end{itemize}

		\textit{Proof}:

		Now we show that such improvement does give us a better policy.
		\begin{align*}
		v^\pi(s)&\le \max_{a}q^\pi(s,a)\\
		&=q^{\pi'}(s,\pi'(s))\\
		&=E_{\pi'}[R_{t+1}+\gamma v^\pi(S_{t+1})|S_t=t]\\
		&\le \mathbb{E}_{\pi'}[R_{t+1}+\gamma q^{\pi'}(S_{t+1},\pi'(S_{t+1}))|S_t=t]\\
		&\le \mathbb{E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2q^{\pi'}(S_{t+2},\pi'(S_{t+2}))|S_t=s]\tag{1}\\
		&\le \mathbb{E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+...|S_t=s]\\
		&=v^{\pi'}(s).
		\end{align*}

		The step (1) can be derived by Adam's law with extra conditioning and Markov theory.
		\qed\\

		When the improvement converges, we have
		\[q^\pi(s,\pi'(s))=\max_{a} q^\pi(s,a)=q^\pi(s,\pi(s))=v^\pi(s),\]
		which gives us the optimal policy.\\

		\begin{algorithm}[H]
		\caption{Policy Iteration}
		\label{alg: Policy Iteration}
		\begin{algorithmic}[1]
        \State initialize a policy $\pi(s)\in\mathcal{A}$ arbitrarily for all $s\in\mathcal{S}$;

        \For {true}:
			\State $v\gets$Policy Evaluation for $\pi$;
        	\State $stable\gets$ true;

        	\For {each $s\in\mathcal{S}$}:
        		\State $old\gets \pi(s)$;
        		\State $\pi(s)\gets\arg\max_{a}\sum_{s'}\mathcal{P}_{ss'}^a[R_s^a+\gamma v(s')]$;
        		
        		\If {$old\ne\pi(s)$}:
            		\State $stable\gets$false;
            	\EndIf
            \EndFor


            \If {$stable$}:
            	\State Return $\pi$\;
            \EndIf
        \EndFor
        \end{algorithmic}
        \end{algorithm}

        It should be noticed that here we are talking about deterministic policy. For stochastic policy, expectation should be introduced.\\

		\textbf{Generalized Policy Iteration}(GPI): \textit{The combination of policy evaluation and policy improvement is called generalized policy iteration.}

		In GPI one maintains both an approximate policy and an approximate value function. The value function is repeatedly altered to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the current value function. These two kinds of changes work against each other to some extent, as each creates a moving target for the other, but together they cause both policy and value function to approach optimality.\\

		%GPI is also the basic idea of Actor-Critic.

	\subsubsection{Value Iteration}

		One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. After the value function converges, the change of the policy will lead to a new round evaluation.

		In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing the convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one update of each state). This algorithm is called \textit{value iteration}. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps.

		\begin{algorithm}[H]
		\caption{Value Iteration}
		\label{alg: Value Iteration}
		\begin{algorithmic}[1]
		\State initialize $v(s)$ arbitrarily for $s\in\mathcal{S}$ except that $v(terminal)=0$;
        \State \textbf{input} the threshold $\theta>0$ determining the accuracy of the evaluation; 
        
        \For {true}:
        	\State $\Delta\gets 0$;
        	\For {each $s\in\mathcal{S}$}:
        		\State $old\gets v(s)$;
        		\State $v(s)\gets\max_{a}\pi(a|s)\sum_{s'}\mathcal{P}_{ss'}^a[R_s^a+\gamma v(s')]$;
        		\State $\Delta\gets\max(\Delta,|old-v(s)|)$;
        	\EndFor

        	\If {$\Delta<\theta$}:
        	\State break;
        	\EndIf
        \EndFor
        \State \textbf{return} $\pi(s)=\arg\max_a\sum_{s'}\mathcal{P}_{ss'}^a[R_s^a+\gamma v(s')]$;
        \end{algorithmic}
        \end{algorithm}

        We can tell that value iteration is different from policy evaluation in how we update $v(s)$. The update rule in value iteration follows \textit{Bellman optimality equation}.\\

		\textbf{Bellman Optimality Equation}: \textit{The optimal value functions are reached by the Bellman optimality equations}:
		\[v^{*}(s)=\max _{a} R_s^a+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} | s, a\right) v^{*}\left(s^{\prime}\right).\]\

		The derivation of the equation is simple. According to the optimal policy, we know
		\[v^{*}(s)=\max_a q^{*}(s, a)\tag{1}.\]
		Then from the definition of $q(s,a)$,
		\[q^{*}(s, a)=R_s^a+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} | s, a\right) v^{*}\left(s^{\prime}\right).\tag{2}\]
		Therefore we can get the Bellman optimality equation by plugging (2) into (1).\\

	\subsubsection{Comparison}
		The comparison of the two methods are shown as follows.\\

		\textit{Policy Iteration}: policy evaluation + policy improvement
		\begin{itemize}[noitemsep,topsep=0pt]
		\item starts from an arbitrarily policy and then estimates the \textit{state value} given the policy;
		\item generates a new policy given the estimated state value;
		\item converges faster in terms of the number of iterations since it doing a lot more work in each iteration.\\
 		\end{itemize}

		\textit{Value Iteration}: optimal value function + one policy extraction
		\begin{itemize}[noitemsep,topsep=0pt]
			\item updates the value \textit{greedily} (does not care the policy) at each iteration and then, \textit{after} finding the optimal value function, determines a new policy given the optimal value function;
  			\item convergences fast per iteration, but may be far from the true value function.\\
		\end{itemize}

		The summary for evaluation and control in MDP by dynamic programming is shown in Table~\ref{dp_comp}.
	
		% Please add the following required packages to your document preamble:
		% \usepackage{multirow}
		\begin{table}[H]
		\centering
		\begin{tabular}{|c|c|c|c|}
		\hline
		Problem                     & \multicolumn{2}{c|}{Bellman Equation} & Algorithm                          \\ \hline
		\multirow{2}{*}{Prediction} & \multirow{2}{*}{Expectation}    & $v^\pi(s)=\mathbb{E}_\pi[R_{t+1}+\gamma v^\pi(S_{t+1})|S_t=s]$   & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Policy \\ Evaluation\end{tabular}} \\ \cline{3-3}
		                            &                                 & $q^\pi(s,a)=\mathbb{E}_\pi[R_{t+1}+\gamma q^\pi(S_{t+1}, A_{t+1})|S_t=t,A_t=a]$   &                                    \\ \hline
		\multirow{2}{*}{Control}    & \multirow{2}{*}{Expectation}    & $v^\pi(s)=\mathbb{E}_\pi[R_{t+1}+\gamma v^\pi(S_{t+1})|S_t=s]$   & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Policy \\ Iteration\end{tabular}}  \\ \cline{3-3}
		                            &                                 & $q^\pi(s,a)=\mathbb{E}_\pi[R_{t+1}+\gamma q^\pi(S_{t+1}, A_{t+1})|S_t=t,A_t=a]$   &                                    \\ \hline
		\multirow{2}{*}{Control}    & \multirow{2}{*}{Optimality}     & $v^\ast(s)=\max_a R_s^a+\gamma\sum_{s'\in \mathcal{S}}P(s'|s,a)v^\ast(s')$   & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Value \\ Iteration\end{tabular}}   \\ \cline{3-3}
		                            &                                 & $q^\ast(s,a)=R_s^a +\gamma\sum_{s'\in \mathcal{S}}P(s'|s,a)\max_{a'}q^\ast(s',a')$   &                                    \\ \hline
		\end{tabular}
		\caption{Dynamic Programming algorithms for MDPs}
		\label{dp_comp}
		\end{table}

\pagebreak

\section{Model-Free Prediction}
	To begin, we firstly give definition of the term \textit{episode} then give the prediction algorithm based MC and TD methods.

	\textbf{Episodes}: \textit{An episode $\tau$ is a sequence of states and actions in the environment,}
	\[\tau=(s_0,a_0,r_1,...,s_{T-1},a_{T-1},r_T).\]
	We call an episode is complete if it ends with the terminal state.\\

	In model free method, we only utilize the episode itself without further exploitation.\\

	\subsection{Monte-Carlo Policy Evaluation} 
		\label{subsec: Monte-Carlo Policy Evaluation}
		Monte-Carlo (MC) policy evaluation methods learn directly from the \textit{complete} episodes, thus it needs no knowledge of MDP transitions or rewards. The limitation of MC policy method methods is that it requires MDPs are episodic.\\

		\textbf{Monte-Carlo Policy Evaluation}: \textit{Given some complete episodes under the policy $\pi$, we can approximate the value of $s$ by the average returns observed after visiting to $s$.}

		Depending on when average returns for state $s$ in an episode, there are two different implementations.\\

		\textbf{First-Visit Monte-Carlo Policy Evaluation}: \textit{Only at the first time-step $t$ that state $s$ is visited in an episodes, we do the following procedure:}
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Increment counter $N(s)\gets N(s)+1$;
			\item Increment the total return $return(s)\gets return(s)+G_t$;
			\item Update the value by the mean return $v(s)\gets return(s)/N(s)$.
		\end{itemize}

		\begin{algorithm}[h]
		\caption{First-Visit Monte-Carlo Policy Evaluation}
		\label{alg: First-Visit Monte-Carlo Policy Evaluation}
		\begin{algorithmic}[1]
			\State initialize $v(s)\in\mathbb{R}$ arbitrarily for all $s\in\mathcal{S}$; 
            \State initialize $return(s)\gets$ an empty list for all $s\in\mathcal{S}$;
            \State \textbf{input} the policy $\pi$ to be evaluated; 
            
            \For {true}:
            	\Statex \# variants for this alg. can be start with $s_0\in\mathcal{S},a_0\in\mathcal{A}(s_0)$ randomly, $\varepsilon-$greedy, \textit{etc.}
            	\State Generate a complete episode $\tau=(s_0,a_0,r_1,...,s_{T-1},a_{T-1},r_{T})$;
            	\State $G\gets0$;
            	\For {$t=T-1,T-2,...0$}:
            		\State $G\gets\gamma G+r_{t+1}$;
            		\If {$s_t$ appears in $(s_0,s_1,...,s_{t-1})$}:
            			\State Append $G$ to $return(s_t)$;
            			\State $v(s_t)\gets$average($return(s_t)$);
            		\EndIf
            	\EndFor
            \EndFor
        \end{algorithmic}
        \end{algorithm}

		\textbf{Every-Visit Monte-Carlo Policy Evaluation}: \textit{Every time-step $t$ that state $s$ is visited in an episode, we do the following procedure:}
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Increment counter $N(s)\gets N(s)+1$;
			\item Increment the total return $return(s)\gets return(s)+G_t$;
			\item Update the value by the mean return $v(s)\gets return(s)/N(s)$.\\
		\end{itemize}

		\begin{algorithm}[H]
		\caption{Every-Visit Monte-Carlo Policy Evaluation}
		\label{alg: Every-Visit Monte-Carlo Policy Evaluation}
		\begin{algorithmic}[1]
  			\State \textbf{input} the policy $\pi$ to be evaluated;
  			\State initialize $v(s)\in\mathbb{R}$ arbitrarily for all $s\in\mathcal{S}$;
			\State initialize $return(s)\gets$ an empty list for all $s\in\mathcal{S}$;
            
            \For {true}:
            	\State Generate a complete episode $\tau=(s_0,a_0,r_1,...,s_{T-1},a_{T-1},r_{T})$;
            	\For {$t=T-1,T-2,...,0$}:
            		\State $G\gets\gamma G+r_{t+1}$;
            		\State Append $G$ to $return(s_t)$;
            		\State $v(s_t)\gets$average($return(s_t)$);
            	\EndFor
            \EndFor
        \end{algorithmic}
        \end{algorithm}

		By \textit{law of large numbers}, both of two methods can achieve $v(s)\to v^\pi(s)$ as $N(s)\to\infty$. In practice, we can use a trick \textit{incremental mean} to simplify the calculation.\\

		Differences between DP and MC for policy evaluation:
		\begin{itemize}[noitemsep,topsep=0pt]
		\item DP computes $v_k$ by bootstrapping the rest of the expected return calculated with $v_{k−1}$;
		\item DP iterates on Bellman expectation backup:
		\[v_k(s)\gets\sum_{a\in \mathcal{A}}\pi(a|s)\left(R_s^a+\gamma\sum_{s'\in \mathcal{S}}P(s'|s,a)v_{k-1}(s')\right).\]
		\item MC updates the empirical mean return with a sampled episode:
		\[v(s_t)\gets v(s_t)+\alpha(G_{k,t} - v(s_t)).\]\
		\end{itemize}

		Advantages of MC over DP:
		\begin{itemize}[noitemsep,topsep=0pt]
		\item MC can work when the environment is unknown;
		\item Working with sampled episodes has a huge advantage. Even with the complete knowledge of the environment's dynamics, the complexity still could be a challenge;
		\item Cost of estimating a single state's value is independent of the total number of states. So one can sample episodes starting from the states of interest then average returns.\\
		\end{itemize}

	\subsection{Temporal-Difference Learning}

		Unlike MC methods, temporal-difference (TD) does not require the episodes are complete. TD methods can learn from incomplete episodes by bootstrapping.\\

		\textbf{Temporal-Difference Learning}: \textit{Given some incomplete episodes under the policy $\pi$, we update value $v(s_t)$ toward estimated return $r_{t+1}+\gamma v(s_{t+1})$:}
		\[v(s_t)\gets v(s_t)+\alpha(r_{t+1}+\gamma v(s_{t+1})-v(s_t)),\]
		\textit{where $r_{t+1}+\gamma v(s_{t+1})$ is called the TD target, $\alpha$ is the step-size, and we call}
		\[\delta_t=r_{t+1}+\gamma v(s_{t+1})-v(s_t)\]
		\textit{the TD error.\\}

		\begin{algorithm}[h]
		\caption{TD(0) Evaluation}
		\label{alg: TD(0) Evaluation}
		\begin{algorithmic}[1]
            \State initialize $v(s)\in\mathbb{R}$ arbitrarily for all $s\in\mathcal{S}$ except that $v(terminal)=0$;
            \State \textbf{input} the policy $\pi$ to be evaluated; the step size $\alpha\in(0,1]$;
            
            \For {true}:
            	\State Generate initial state $s$;
            	\While {$s$ is not terminal}:
            		\State $a\gets\pi(s)$;
            		\State $r,s'\gets environment(s,a)$;
            		\State $v(s)\gets v(s)+\alpha(r+\gamma v(s')-v(s))$;
            		\State $s\gets s'$;
            	\EndWhile
            \EndFor
        \end{algorithmic}
        \end{algorithm}

		Differences between MC and TD for policy evaluation:
		\begin{itemize}[noitemsep,topsep=0pt]
			\item TD can learn online after every step;
			\item MC must wait until end of episode before return is known;
			\item TD can learn from incomplete sequences;
			\item MC can only learn from complete sequences;
			\item TD works in continuing (non-terminating) environments;
			\item MC only works in episodic (terminating) environments;
			\item TD exploits Markov property, and it is efficient in Markov environments;
			\item MC does not exploit Markov property, thus it is relatively effective in non-Markov environments.\\
		\end{itemize}
		
		\textit{Bootstrapping}: involves old values, it is something like in-place update. 

		\textit{Sampling}: samples to get an expectation. 

		A summary of \textit{bootstrapping} and \textit{sampling} for DP, MC, and TD is shown in Table~\ref{boot_samp}. 
		\begin{table}[H]
		\centering
		\begin{tabular}{|l|l|l|}
		\hline
		Algorithm           & Bootstraps & Sampling \\\hline
		Dynamic Programming & $\surd$          &         \\\hline
		Monte-Carlo         &           & $\surd$        \\\hline
		Temporal-Difference & $\surd$          & $\surd$     \\\hline
		\end{tabular}
		\caption{\textit{Bootstrapping} and \textit{sampling} for DP, MC, and TD.}
		\label{boot_samp}
		\end{table}

		\textbf{n-step TD methods}: \textit{Unlike the simplest TD method, in $n$-step TD methods, the updating rule of value $v(s_t)$ is}
		\[v(s_t)\gets v(s_t)+\alpha(G_t^{(n)}-v(S_t)),\]
		where $G_t^{(n)}$ is the $n$-step return
		\[G_t^{(n)}=r_{t+1}+\gamma r_{t+2}+...+\gamma ^nv(s_{t+n}).\]

		Notice that with additional definition, we can generalize TD to MC when $n\to\infty$.\\

		\textbf{TD($\lambda$) methods}: \textit{To make use of the information from all time-steps, we can use weight $(1-\lambda)\lambda^{n-1}$ to average $n$-step returns over different $n$ as}
		\[G_t^\lambda=(1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}G_t^{(n)},\]
		\textit{thus the updating rule for the value becomes}
		\[v(s_t)\gets v(s_t)+\alpha(G_t^\lambda - v(s_t)).\]

\pagebreak

\section{Model-Free Control}

	In the last section, we introduce Monte-Carlo (MC) and Temporal Difference (TD) methods to evaluate the value function of a policy. With the idea of generalized policy iteration (GPI) we mentioned in section~\ref{sec: PI}, we now consider how the value function can be used in control, that is, to find the optimal policy.

	For an MDP with large state space, the way we sample episodes matters a lot. We define \textit{behavior policy} that determines which action to take and \textit{target policy} that determines the best policy we have so far, then based on the two concepts, we have two learning form:
	\begin{itemize}[noitemsep,topsep=0pt]
		\item \textbf{On-policy} learning: the target policy and the behavior policy are the same, which means the action we will take follows the optimal policy we find;
		\item \textbf{Off-policy} learning: the target policy and the behavior policy are different, which means the action we will take is independent to the optimal policy we find.
	\end{itemize}

	On-policy may not ensure the enough exploration of the state space as when we update the policy greedily we may discard those states with great potential. Compared with on-policy learning, off-policy learning is more powerful and general, though it is often of greater variance and is slower to converge.\\

	\subsection{On Policy Monte-Carlo Control}

			Like MC evaluation we mentioned in section~\ref{subsec: Monte-Carlo Policy Evaluation}, MC control also requires fully episodes to improve the policy. It is natural to alternate between evaluation and improvement on an episode-by-episode basis. The following algorithms are based on the implementation of MC, and the differences lie in how they generate samples.

			\textbf{Exploring Starts}: \textit{To obtain diverse episodes, a naive ideal is initializing each episode randomly:}
			
			\begin{algorithm}[h]
			\caption{Monte-Carlo Control}
			\label{alg: Monte-Carlo Control}
			\begin{algorithmic}[1]
			\State initialize $\pi(s)\in \mathcal{A}(s)$(arbitrarily), for all $s\in \mathcal{S}$; initialize $return(s)\gets$ an empty list for all $s\in\mathcal{S}$;
	        
            \For {true}:
            	\Statex \# variants for this alg. can be start with $s_0\in\mathcal{S},a_0\in\mathcal{A}(s_0)$ randomly, $\varepsilon-$greedy, \textit{etc.}
            	\State Generate a complete episode $\tau=(s_0, a_0, r_1,...,s_{T-1},a_{T-1},r_{T})$;
            	\State $G\gets0$;
            	\For {$t=T-1,T-2,...0$}:
            		\State $G\gets\gamma G+r_{t+1}$;
            		\If {$s_t,a_t$ appears in $(s_0,a_0,s_1,a_1,...,s_{t-1},a_{t-1})$}:
            			\State Append $G$ to $return(s_t, a_t)$;
            			\State $q(s_t,a_t)\gets$average($return(s_t,a_t)$);
            			\State $\pi(s_t)\gets\arg\max_{a'}q(s_t,a')$;
            		\EndIf
            	\EndFor
            \EndFor
	        \end{algorithmic}
	        \end{algorithm}

			Random starts usually are not common in reality. In the iteration part, we can leverage \textit{$\varepsilon-$Greedy Exploration} to avoid the unlikely assumption of exploring starts.

			\textbf{$\varepsilon-$Greedy Exploration}: \textit{For a state with $m$ actions, there is non-zero probability to try them:}
			\[\pi(a|s)=\begin{cases}\frac{\varepsilon}{m}+1-\varepsilon, & a=\arg\max_{a'\in\mathcal{A}}q(s,a')\\\frac{\varepsilon}{m}, & \text{ otherwise}\end{cases}\]
			\textit{which means we will choose the greedy action with probability $1-\varepsilon$ and choose an action at random with probability $\varepsilon$.}

			\textbf{Policy improvement theorem}: \textit{For any $\varepsilon$-greedy policy $\pi$, the $\varepsilon$-greedy policy $\pi'$ with respect to $q^\pi$ is an improvement, i.e., $v_\pi'(s)\ge v_\pi(s)$ for all $s\in\mathcal{S}$.}

			\textit{Proof}:
			\begin{align*}
			q^{\pi}\left(s, \pi^{\prime}(s)\right) &=\sum_{a \in \mathcal{A}} \pi^{\prime}(a | s) q^{\pi}(s, a) \\
			&=\frac{\varepsilon}{m} \sum_{a \in \mathcal{A}} q^{\pi}(s, a)+(1-\varepsilon) \max _{a} q^{\pi}(s, a) \\
			& \geq \frac{\varepsilon}{m} \sum_{a \in \mathcal{A}} q^{\pi}(s, a)+(1-\varepsilon) \sum_{a \in \mathcal{A}} \frac{\pi(a | s)-\frac{\varepsilon}{m}}{1-\varepsilon} q^{\pi}(s, a) \tag{1}\\
			&=\sum_{a \in \mathcal{A}} \pi(a | s) q^{\pi}(s, a)\\
			&=v^{\pi}(s).
			\end{align*}
			\qed

			(The above proof is given in the Chapter 5 of the book~\cite{sutton2018reinforcement}. After expanding the sum, however, it can be shown the inequality should be equality. Hmm...)\\

			For $\varepsilon$-greedy, when we explore, we choose actions at random without regard to their estimated values, which may waste a chance on the action that we can sure it has a bad performance. To focus more on those states with higher value, we can refer to \textit{Boltzmann exploration}.

			\textbf{Boltzmann Exploration}: \textit{Boltzmann exploration also known as Gibbs sampling and soft-max, it chooses an action based on its estimated value:}
			\[\pi(a|s)=\frac{e^{\beta\hat q(s,a)}}{\sum_{a'}e^{\beta\hat q(s,a')}},\]
			\textit{where $\hat q(s,a)$ is the estimation of the value of being in state $s$ and taking action $a$, $\beta$ is a tunable parameter.}\\

	\subsection{On Policy Temporal-Difference Control: Sarsa}

			As we mentioned before, Temporal-Difference(TD) learning has several advantages over Monte-Carlo (MC) such as lower variance, online, incomplete sequences. The difference between TD control and TD learning is similar to that between MC control and MC learning. What we need to do is applying TD to $Q(S,A)$ and updating the policy every time-step.\\

			\textbf{Sarsa}: \textit{The name Sarsa is inspired by the exploitation on the quintuple $s_t,a_t,r_{t+1},s_{t+1},a_{t+1}$. Consider transitions from state-action pair to state-action pair, and learn the values of state-action pairs:}
			\[q(s_t,a_t)\gets q(s_t,a_t)+\alpha[r_{t+1}+\gamma q(s_{t+1},a_{t+1})-q(s_t,a_t)].\]

			\begin{algorithm}[h]
			\caption{TD Sarsa}
			\label{alg: TD Sarsa}
			\begin{algorithmic}[1]
			\State initialize $\pi(s)\in \mathcal{A}(s)$(arbitrarily), for all $s\in \mathcal{S}$; initialize $q(s,a)\in\mathcal{R}$(arbitrarily) for all possible pairs in $\mathcal{S}\times\mathcal{A}$; initialize $q(terminal,\cdot)=0$;
	        
            \For {true}:
            	\Statex \# variants for this alg. can be start with $s_0\in\mathcal{S},a_0\in\mathcal{A}(s_0)$ randomly, $\varepsilon-$greedy, \textit{etc.}
            	\State Generate a start state $s$;
            	\State Choose action $a\gets\pi(s)$;
            	\While {$s$ is not terminal}:
            		\State $r,s'\gets environment(s,a)$;
            		\State $a'\gets \pi(s')$
            		\State $q(s,a)\gets q(s,a)+\alpha[r+\gamma q(s',a')-q(s,a)]$;
            		\State Update $\pi$ according to $q(s,a)$;
            		\State $s\gets s'$;
            		\State $a\gets a'$;
            	\EndWhile
            \EndFor
	        \end{algorithmic}
	        \end{algorithm}

			There are also $n$-step Sarsa and Sarsa($\lambda$) which can be derived from TD methods.

            \textbf{$n$-step Sarsa}: \textit{Define the $n$-step $Q$-return as}
            	\[q^{(n)}_t=r_{t+1}+\gamma r_{t+2}+...+\gamma^{n-1}r_{t+n}+\gamma^n q(s_{t+n},a_{t+n}),\]
            \textit{then $n$-step Sarsa updates $q(s,a)$ towards the $n$-step $Q$-return}
            	\[q(s_t,a_t)\gets q(a_t,t_t)+\alpha (q_t^{(n)}-q(s_t,a_t)).\]
            Like what we mentioned in $n$-step TD learning, when $n\to\infty$, Sarsa$\to$MC.\\

	\subsection{Off-Policy Temporal-Difference Control: Q-Learning}

			The development of Q-learning is a big breakout in the early days of Reinforcement Learning.

			For the target policy, Q-learning updates it in a \textit{greedy} way:
				\[\pi(s_{t+1})=\arg\max_{a'}q(s_{t+1},a').\]
			
			For the behavior policy, it will be updated in an $\varepsilon$-\textit{greedy} way on $q(s,a)$. 

			The state-action value will be updated as
			\[q(s,a)\gets q(s,a)+\alpha[r+\gamma\max_{a'}q(s',a')-q(s,a)].\]

			\begin{algorithm}[H]
			\caption{Q-Learning}
			\label{alg: Q-Learning}
			\begin{algorithmic}[1]
			\State initialize $\pi(s)\in \mathcal{A}(s)$(arbitrarily), for all $s\in \mathcal{S}$; initialize $q(s,a)\in\mathcal{R}$(arbitrarily) for all possible pairs in $\mathcal{S}\times\mathcal{A}$; initialize $q(terminal,\cdot)=0$;
	        
            \For {true}:
            	\Statex \# variants for this alg. can be start with $s_0\in\mathcal{S},a_0\in\mathcal{A}(s_0)$ randomly, $\varepsilon-$greedy, \textit{etc.}
            	\State Generate a start state $s$;
            	\While {$s$ is not terminal}:
					\State Choose action $a\gets\pi(s)$;
            		\State $r,s'\gets environment(s,a)$;
            		\State $q(s,a)\gets q(s,a)+\alpha[r+\gamma\max_{a'} q(s',a')-q(s,a)]$;
            		\State Update $\pi$ according to $q(s,a)$;
            		\State $s\gets s'$;
            	\EndWhile
            \EndFor
	        \end{algorithmic}
	        \end{algorithm}

            One should notice that different with TD Sarsa in line 8, the action Q-learning uses to update $q(s,a)$ in line 7 is not the same as the one it truly takes.\\
			\begin{comment}
            The comparison of Sarsa and Q-Learning is shown as follows:

            Sarsa: On-Policy TD control
            \begin{itemize}[noitemsep,topsep=0pt]
            	\item Choose action $A_t$ from $S_t$ using policy derived from $Q$ with $\varepsilon$-greedy;
            	\item Take action $A_t$, observe $R_{t+1}$ and $S_{t+1}$;
            	\item Choose action $A_{t+1}$ from $S_{t+1}$ using policy derived from $Q$ with $\varepsilon$-greedy;
            	\item $Q(S_t,A_t)\gets Q(S_t,A_t)+\alpha[R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]$.
            \end{itemize}

            Q-Learning: Off-Policy TD control
            \begin{itemize}[noitemsep,topsep=0pt]
            	\item Choose action $A_t$ from $S_t$ using policy derived from $Q$ with $\varepsilon$-greedy;
            	\item Take action $A_t$, observe $R_{t+1}$ and $S_{t+1}$;
            	\item 'Imagine' $A_{t+1}$ as $\arg\max_{a'}Q(S_{t+1},a')$ in the update target; 
            	\item $Q(S_t,A_t)\gets Q(S_t,A_t)+\alpha[R_{t+1}+\gamma \max_{a'}Q(S_{t+1},a')-Q(S_t,A_t)]$.
            \end{itemize}
            		\subsection{Off-Policy Monte-Carlo Control: Importance Sampling}

			Off-Policy control allows us to learn from observing humans or other agents, re-use experience generated from old policies $\pi_1,\pi_2,...,\pi_t$, learn about optimal policy while following exploratory policy and learn about multiple policies while following one policy.

			\textbf{Importance Sampling for Off-Policy Monte-Carlo}: \textit{Let $G_t$ be the returns generated from $\mu$. Define the corrected return}
			\[G_t^{\pi/\mu}=\frac{\pi(A_t|S_t)\pi(A_{t+1}|S_{t+1})...\pi(A_{\tau}|S_{\tau})}{\mu(A_t|S_t)\mu(A_{t+1}|S_{t+1})...\mu(A_{\tau}|S_{\tau})}G_t,\]
			\textit{then we update value by the corrected return}
			\[V(S_t)\gets V(S_t)+\alpha(G_t^{\pi/\mu}-V(S_t)).\]

			By the update rule, we know the $\mu$ can not be zero when $\pi$ is non-zero. It should be noticed that importance sampling can dramatically increase the variance.

			\textbf{Importance Sampling for Off-Policy TD}: \textit{Weight TD target $R+\gamma V(S')$ by importance sampling, then we only need a single importance sampling correction to update the value}
			\[V(S_t)\gets V(S_t)+\alpha\left(\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}(R_{t+1}+\gamma V(S_{t+1}))-V(S_t)\right).\]
			\end{comment}

\pagebreak

\section{Value Function Approximation}
	\label{sec: value function approximation}

	For some problems, the state spaces may huge or even infinity, \textit{e.g.}:
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Backgammon: $10^{20}$ states;
			\item Chess: $10^{47}$ states;
			\item Came of Go: $10^{170}$ states;
			\item Robot Arm and Helicopter: continuous state space;
		\end{itemize}
	
	How can we scale up the model-free methods for these problems?\\

	A solution for these is \textit{function approximation}, which is an instance of \textit{supervised learning}. Concretely, it follows that:
		\begin{itemize}[noitemsep,topsep=0pt]
			\item $\hat v(s;\bm{w})\approx v^\pi(s)$,
			\item $\hat q(s,a;\bm{w})\approx q^\pi(s,a)$,
			\item $\hat \pi(s,a;\bm{w})\approx \pi(a|s)$,
		\end{itemize}
	where $\bm{w}$ is the parameter to be learned. With such approximation, we can generalize from seen states to unseen states. Obviously, we have many available function approximator:
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Linear combinations of features,
			\item Neural networks,
			\item Decision trees,
			\item Nearest neighbors.
		\end{itemize}
	Among those, we will focus on the first two as they are differentiable and thus easy to be optimized. It is worth to notice that though such generalization makes the learning potentially more powerful (it may even applicable to partially observable problems), it is potentially more difficult to manage and understand.\\

	\subsection{Semi-gradient Method}

			Now we consider a naive case where we have an oracle for knowing the true value $v^\pi(s)$ for any given state $s$ under policy $\pi$. Then the object is to find the estimator $\hat{v}(s,\bm{w})$ approximating $v^\pi(s)$. We can define the loss function by the mean squared error:
			\[J(\bm{w})=\mathbb{E}_\pi[(v^\pi(s)-\hat v(s,\bm{w}))^2],\]
			and the gradient descend for the loss function is:
			\[\Delta\bm{w}=-\frac{1}{2}\alpha \nabla_{\bm{w}} J(\bm{w}),\]
			\[\bm{w}_{t+1}=\bm{w}_t+\Delta\bm{w}.\]
			Follow the gradient descend, we can find a local minimum. Further, if the value function is represented by a linear combination of features, then such method can converge to the global optimum.\\

			\subsubsection{Semi-gradient Method for Prediction} % (fold)

				However, in practice, no access to the oracle of the true value $v^\pi(s)$. What we have is only the reward, or, the bootstrapping estimation. Thus we can substitute the target for $v^\pi(s)$:
				\begin{itemize}[noitemsep,topsep=0pt]
					\item For MC, the target is the actual return $G_t$ and hence
					\[\Delta \bm{w}=\alpha(G_t-\hat{v}(s_t,\bm{w}))\nabla_{\bm{w}} \hat{v}(s_t,\bm{w});\]
					\item For TD(0), the target is the TD target $R_{t+1}+\gamma\hat{v}(s_{t+1},\bm{w})$ and hence
					\[\Delta \bm{w}=\alpha(r_{t+1}+\gamma\hat{v}(s_{t+1},\bm{w})-\hat{v}(s_t,\bm{w}))\nabla_{\bm{w}} \hat{v}(s_t,\bm{w})\]
				\end{itemize}

				For TD(0), the gradient is also called as semi-gradient, as the target $r_{t+1}+\gamma\hat{v}(s_{t+1},\bm{w})$ depends on the current value of the weight vector $\bm{w}$ and we just ignore such effect.

				\begin{algorithm}[h]
				\caption{Gradient Monte Carlo Policy Evaluation}
				\label{alg: Gradient Monte-Carlo Policy Evaluation}
				\begin{algorithmic}[1]
					\State initialize the differentiable function $\hat{v}(s,\bm{w})\in\mathbb{R}$ arbitrarily for all $s\in\mathcal{S}$; 
		            \State \textbf{input} the policy $\pi$ to be evaluated; the step size $\alpha$;
		            
		            \For {true}:
		            	\Statex \# variants for this alg. can be start with $s_0\in\mathcal{S},a_0\in\mathcal{A}(s_0)$ randomly, $\varepsilon-$greedy, \textit{etc.}
		            	\State Generate a complete episode $\tau=(s_0,a_0,r_1,...,s_{T-1},a_{T-1},r_{T})$;
		            	\For {$t=0,1,...,T-1$}:
		            		\State $\bm{w}\gets\bm{w}+\alpha[G_t-\hat{v}(s_t,\bm{w})]\nabla \hat{v}(s_t,\bm{w})$;
		            	\EndFor
		            \EndFor
		        \end{algorithmic}
		        \end{algorithm}

		        \begin{algorithm}[h]
				\caption{Semi-gradient TD(0) Policy Evaluation}
				\label{alg: Semi-gradient TD(0) Policy Evaluation}
				\begin{algorithmic}[1]
					\State initialize the differentiable function $\hat{v}(s,\bm{w})\in\mathbb{R}$ arbitrarily for all $s\in\mathcal{S}$; 
		            \State \textbf{input} the policy $\pi$ to be evaluated; the step size $\alpha$;
		            
		            \For {true}:
		            	\State Generate a start state $s$;
		            	\While {$s$ is not terminal}:
		            		\State Choose action $a\gets\pi(s)$;
		            		\State $r,s'\gets environment(s,a)$;
		            		\State $\bm{w}\gets\bm{w}+\alpha[r+\gamma\hat{v}(s',\bm{w})-\hat{v}(s,\bm{w})]\nabla \hat{v}(s,\bm{w})$;
		            		\State $s\gets s'$;
		            	\EndWhile
		            \EndFor
		        \end{algorithmic}
		        \end{algorithm}


			\subsubsection{Semi-gradient Method for Control} % (fold)

				The extension of the semi-gradient prediction methods to state-action values is straightforward. In this case it is the approximate action-value function, $\hat q\approx q^\pi$, that is represented as a parameterized functional form with weight vector $\bm{w}$. The control version is derived from the \textit{generalized policy iteration}. It quite like the way we make prediction:
				\begin{itemize}[noitemsep,topsep=0pt]
					\item For MC, the target is return $G_t$
						\[\Delta \bm{w}=\alpha(G_t-\hat{q}(s_t,a_t,\bm{w}))\nabla \hat{q}(s_t,a_t,\bm{w});\]
					\item For Sarsa, the target is TD target $r_{t+1}+\gamma\hat{q}(s_{t+1},a_{t+1},\bm{w})$:
						\[\Delta \mathbf{w}=\alpha\left(r_{t+1}+\gamma \hat{q}\left(s_{t+1}, a_{t+1}, \mathbf{w}\right)-\hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right);\]
					\item For Q-learning, the target is TD target $r_{t+1}+\gamma \max _{a} \hat{q}\left(s_{t+1}, a, \mathbf{w}\right)$:
						\[\Delta \mathbf{w}=\alpha\left(r_{t+1}+\gamma \max _{a} \hat{q}\left(s_{t+1}, a, \mathbf{w}\right)-\hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right)\right) \nabla_{\mathbf{w}} \hat{q}\left(s_{t}, a_{t}, \mathbf{w}\right).\]
				\end{itemize}

				One can get the corresponding control algorithm by modifying the update rule in the prediction version and then extracting the optimal policy according to $\hat{q}$, which is $\pi(s)=\arg\max_a \hat{q}(s,a,\bm{w})$.\\

	            Though TD control is powerful and practical in most case, there is no guarantees in the convergence of TD. Firstly, TD with VFA follows the \textit{semi-gradient} rather than the true gradient of any objective function. Secondly, the updates involve doing an approximate Bellman backup (model-free) followed by fitting the underlying value function (approximation). Further, for off-policy, behavior policy and target policy are not identical, thus value function approximation may diverge.\\

	            The Deadly Triad for \textit{Danger of Instability and Divergence}:
	            \begin{itemize}[noitemsep,topsep=0pt]
	            	\item Function approximation: A scalable way of generalizing from a state space much larger than the memory and computational resources;
					\item Bootstrapping: Update target that includes existing estimates (as in dynamic programming or TD methods) rather than relying exclusively on actual rewards and complete returns (as in MC methods);
					\item Off-policy learning: training on a distribution of transitions other than that produced by the target policy.\\
	        	\end{itemize}

	        	\begin{algorithm}[h]
				\caption{Semi-gradient TD(0) Policy Evaluation}
				\label{alg: Semi-gradient TD(0) Policy Evaluation}
				\begin{algorithmic}[1]
					\State initialize the differentiable function $\hat{v}(s,\bm{w})\in\mathbb{R}$ arbitrarily for all $s\in\mathcal{S}$; 
		            \State \textbf{input} the policy $\pi$ to be evaluated; the step size $\alpha$;
		            
		            \For {true}:
		            	\State Generate a start state $s$;
		            	\While {$s$ is not terminal}:
		            		\State Choose action $a\gets\pi(s)$;
		            		\State $r,s'\gets environment(s,a)$;
		            		\State $\bm{w}\gets\bm{w}+\alpha[r+\gamma\hat{v}(s',\bm{w})-\hat{v}(s,\bm{w})]\nabla \hat{v}(s,\bm{w})$;
		            		\State $s\gets s'$;
		            	\EndWhile
		            \EndFor
		        \end{algorithmic}
		        \end{algorithm}

	\begin{comment}
		\subsection{Batch Methods} % (fold)

			The incremental method or, without loss of generalization, on-policy learning requires the agent gradually gathers experience in the environment. When the agent cannot interact further with the environment, then we need to learn from the limited date to find the best fitting value function. To achieve that, a trick named \textit{experience replay} is used.

			\textbf{Experience Replay}: \textit{It allows reusing samples from a different behavior policy.}

			Experience replay is sample efficient since any experience can be used. However, it usually introduces a bias when used with a relay buffer, as the trajectories are usually not obtained solely under the current policy.

			\subsubsection{Least Squares Prediction}

				Given value function approximation $\hat{v}(s,\bm{w})$ and the experience $\mathcal{D}$ consisting of $\langle s, v^\pi \rangle$ pairs, \textit{Least Squares} algorithm try to find the best $\bm{w}$ to minimize the sum-squared error between the approximation $\hat{v}(s,\bm{w})$ and the target value $\hat{v}^\pi(s)$:
				\[\min_{\bm{w}} \sum_{s, v^\pi(s)\in\mathcal{D}}(v^\pi(s)-\hat{v}(s,\bm{w}))^2.\]
				Referring to SGD, we can apply stochastic gradient descent update as
					\[\Delta\bm{w}=\alpha(v^\pi-\hat{v}(s,\bm{w}))\nabla_{\bm{w}}\hat{v}(s,\bm{w}),\]
				where we sample state, value from experience as
					\[\langle s, v^\pi\rangle\sim \mathcal{D}.\]

			\subsubsection{Least Squares Control}

				For policy evaluation, we want to efficiently use all experience. For control, we also want to improve the policy. However, the experience $\mathcal{D}$ may be gathered from many policies. So to evaluate $q^\pi(S,A)$ we must learn off-policy. One way is to use the same idea as Q-Learning:
				\begin{itemize}[noitemsep,topsep=0pt]
					\item Use experience generated by old policies
					\[S_t,A_t,R_{t+1},S_{t+1}\sim \pi_{\text{old}};\]
					\item Consider the alternative successor action
					\[A'=\pi_{\text{new}}(S_{t+1});\]
					\item Update $\hat{q}$ towards the value of the alternative action
					\[\hat{q}(S_t,A_t,\bm{w})\sim R_{t+1}+\gamma\hat{q}(S_{t+1},A',\bm{w}).\]
				\end{itemize}

		\label{sub:batch_methods}
		
		% subsection batch_methods (end)
	\end{comment}
	\subsection{Deep Q-Learning} % (fold)

		Deep Q-learning leverages nonlinear function approximator, deep neural networks, to avoid manual designing of features. Such method is also called DQN, and it is a well-known case of \textit{deep reinforcement learning}.

		Deep Reinforcement Learning:
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Frontier in machine learning and artificial intelligence;
			\item Deep neural networks can be used to represent value function, policy function, and even model;
			\item Optimize loss function by stochastic gradient descent.\\
		\end{itemize}

		The success of DQN lies in two innovative techniques that greatly improve and stabilize the training procedure.\\

		\textbf{Experience Replay}
		\begin{itemize}[noitemsep,topsep=0pt]
			\item To reduce the correlations among samples, DQN stores transition $(s_t,a_t,r_t,s_{t+1})$ in a replay memory $\mathcal{D}$;
			\item To perform experience replay, DQN repeats the following
			\begin{itemize}[noitemsep,topsep=0pt]
			\item samples an experience tuple from the dataset: 
			\[(s,a,r,s') \sim \mathcal{D};\]
			\item computes the target value with the sampled tuple: $r + \gamma\max_{a'} \hat{q}(s', a', \bm{w})$;
			\item uses stochastic gradient descent to update the network weights
			\[\Delta\bm{w}=\alpha(r+\gamma\max_{a'}\hat{q}(s',a',\bm{w})-q(s,a,\bm{w}))\nabla_{\bm{w}}\hat{q}(s,a,\bm{w}).\]
			\end{itemize}
		\end{itemize}

		\textbf{Fixed Target}
		\begin{itemize}[noitemsep,topsep=0pt]
			\item To help improve stability, DQN fixes the weights used in the target calculation for multiple updates;
			\item Let a different set of parameter $\bm{w}^−$ be the set of weights used in the target, and $\bm{w}$ be the weights to be updated in each step;
			\item To perform experience replay with fixed target, DQN repeats the following
			\begin{itemize}[noitemsep,topsep=0pt]
				\item samples an experience tuple from the dataset: $(s,a,r,s') \sim \mathcal{D}$;
				\item computes the target value for the sampled tuple:
				\[r+\gamma\max_{a'}\hat{q}(s',a',\bm{w}^-);\]
				\item uses stochastic gradient descent to update the network weights:
				\[\Delta\bm{w}=\alpha(r+\gamma\max_{a'}\hat{q}(s',a',\bm{w}^-)-q(s,a,\bm{w}))\nabla_{\bm{w}}\hat{q}(s,a,\bm{w});\]
				\item updates $\bm{w}^-\gets\bm{w}$ periodically.
			\end{itemize}
		\end{itemize}

		\begin{algorithm}[h]
				\caption{Deep-Q Network}
				\label{alg: Deep-Q Network}
				\begin{algorithmic}[1]
					\State initialize the replay memory $\mathcal{D}$ with capacity $N$; initialize the state-action value function $q$ with random weight $\bm{w}$; initialize the target state-action value function $\hat q$ with random weight $\bm{w}^-=\bm{w}$;
		            \State \textbf{input} the period $C$;

		            \For {true}:
		            	\State Generate a start state $s$;
		            	\While {$s$ is not terminal}:
		            		\State Choose action $a=\begin{cases}\arg\max_{a'\in\mathcal{A}}q(s,a'), & \text{with probability } \varepsilon;\\ \text{a random action}, & \text{otherwise};\end{cases}$
		            		\State $r,s'\gets environment(s,a)$;
		            		\State Store transition $(s,a,r,s')$ in $\mathcal{D}$;
		            		\State Sample random minibatch of transitions $(s_j,a_j,r_j,s_{j+1})$ from $\mathcal{D}$;
		            		\State Set $y_j=\begin{cases}r_j, &\text{if }s_{j+1}\text{ is the terminal state};\\ r_j+\gamma\max_{a'}\hat{q}(s_{j+1},a',\bm{w}^-), &\text{otherwise};\end{cases}$
		            		\State Perform a gradient descent step on $\left(y_j-q(s_j,a_j,\bm{w})\right)^2$ with respect to the parameter $\bm{w}$;
		            		\State Reset $\hat{q}=q$ every $C$ steps;
		            	\EndWhile
		            \EndFor
		        \end{algorithmic}
		        \end{algorithm}

	\label{sub:deep_q_networks}
	
	% subsection deep_q_networks (end)

\pagebreak
	
\section{Policy Optimization}

	For value-based reinforcement learning, deterministic policy is generated directly from the value function using greedy $a=\arg\max_{a'} q(a',s)$. Now instead we can parameterize the policy function as $\pi_{\theta}(a|s)$ where $\theta$ is the learnable policy parameter and the output is a probability over the action set.\\

	Value-based \textit{v.s.} Policy-based
		\begin{itemize}[noitemsep,topsep=0pt]
			\item Value-based methods: solve RL problems through dynamic programming
			\begin{itemize}[noitemsep,topsep=0pt]
				\item related to classic RL and control theory;
				\item learn value function;
				\item generate an implicit policy based on the value function;
				\item learn a deterministic policy based on the estimated action values;
				\item developed by Richard Sutton, David Silver, DeepMind;
			\end{itemize}
			\item Policy-based methods: solve RL problems mainly through learning
		 	\begin{itemize}[noitemsep,topsep=0pt]
		 		\item related to machine learning and deep learning;
				\item do not require value function for action selection;
				\item learn a stochastic policy;
				\item developed by Pieter Abbeel, Sergey Levine, OpenAI, Berkeley;
			\end{itemize}
		\end{itemize}
	The two methods can also be combined together. A popular algorithm called \textit{Actor-Critic} entails learning both policy and value function.\\

	Pros and cons of Policy-based
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Advantages:
		\begin{itemize}[noitemsep,topsep=0pt]
			\item can converge on a local optimum (worst case) or global optimum (best case);
			\item is effective in high-dimensional action space;
		\end{itemize}
		\item Disadvantages:
		\begin{itemize}[noitemsep,topsep=0pt]
			\item typically converges to a local optimum;
			\item the policy is of high variance;\\
		\end{itemize}
	\end{itemize}

	\subsection{Policy Optimization Theorem}

		\textbf{Objective of Optimization Policy}: \textit{Given a policy approximator $\pi_θ(s,a)$ with parameter $\theta$, find the $\theta^\star$ that gives us the optimal policy.}

		One thing we care is how do we measure the quality of a policy $\pi_\theta$. Let $\tau$ be a trajectory sampled from the policy function $\pi_\theta$, then we defined the value of policy $\pi_\theta$ as
		\[J(\theta)=\mathbb{E}_{\tau}\left[\sum_t r(s_t, a_t^\tau)\right].\]
		Thus we have the goal of policy-based methods as
		\[\theta^\ast =\arg\max_{\theta}\mathbb{E}_{\tau}\left[\sum_t r(s_t, a_t^\tau)\right].\]
		However, such $J(\theta)$ may not available or handy. Hence a trick is using approximation. For example,
		\begin{itemize}[noitemsep,topsep=0pt]
			\item In the episodic environment with discrete space, we can use the value of the starting state $s_0$:
			\[J(\theta)\approx v^{\pi_\theta}(s_0)=\mathbb{E}_{\pi_\theta}[v(s_0)],\]
			\item In the environment with continuous space, we can use the average reward:
			\[J(\theta)\approx \sum_s d^{\pi_\theta}(s)v^{\pi_\theta}(s)=\sum_{s}d^{\pi_\theta}\sum_{a\in\mathcal{A}}\pi_\theta(a|s)q^{\pi_\theta}(s,a),\]
			where $d^{\pi_\theta}$ is the stationary distribution of Markov chain for $\pi_\theta$.\\
		\end{itemize}

		Depends on the form of $J(\theta)$, we have different methods to maximize it
		\begin{itemize}[noitemsep,topsep=0pt]
			\item If $J(\theta)$ is differentiable, we can use gradient-based methods:
			\begin{itemize}[noitemsep,topsep=0pt]
				\item Gradient Ascend;
				\item Conjugate Gradient;
				\item Quasi-newton.
			\end{itemize}
			\item If $J(\theta)$ is non-differentiable or hard to compute the derivative, we can use some derivative-free black-box optimization methods:
			\begin{itemize}[noitemsep,topsep=0pt]
				\item Cross-entropy Method (CEM);
				\item Hill Climbing;
				\item Evolution Algorithm;
				\item Approximate Gradients by Finite Difference.
			\end{itemize}
		\end{itemize}
		In this note, we mainly focus on gradient-based methods.\\

		\textbf{Policy Gradient Theorem}: \textit{For a policy $\pi_\theta$ with parameter $\theta$, we have the gradient as}
		\[\nabla_\theta J(\theta)=\mathbb{E}_{\pi_\theta}[\nabla_\theta \ln \pi(a|s;\theta) q^\pi(s,a)].\]
		The proof is available in the Section 12.1 of \textit{Reinforcement Learning: An Introduction}~\cite{sutton2018reinforcement}. A detailed proof is also given in the blog~\cite{liblog-pga}. The following \textit{proof} is just for personal interest.

		\textit{Proof}:

		For simplicity, we leave it implicit in all cases that $\pi$ is a function of $\theta$ and all gradients are also implicitly with respect to $\theta$. Notice that the gradient of the state-value function:
		\begin{align*}
			\nabla v^\pi(s)&=\nabla \left(\sum_{a} \pi(a|s)q^\pi(s,a)\right)\\
			&=\sum_a\left(\nabla \pi(a|s)q^\pi(s,a)+\pi(a|s)\nabla q^\pi(s,a)\right)\tag{Product rule of calculus}\\
			&=\sum_a\left(\nabla\pi(a|s)q^\pi(s,a)+\pi(a|s)\nabla\left(R_s^a+\sum_{s'}\mathcal{P}_{ss'}^a v^\pi(s')\right)\right)\tag{Sec 5.2 MDP}\\
			&=\sum_a\left(\nabla\pi(a|s)q^\pi(s,a)+\pi(a|s)\nabla\sum_{s'}\mathcal{P}_{ss'}^av^\pi(s')\right)\tag{\(R_s^a\) is irrevalent to \(\theta\)}\\
			&=\sum_a\left(\nabla\pi(a|s)q^\pi(s,a)+\pi(a|s)\sum_{s'}\mathcal{P}_{ss'}^a\nabla v^\pi(s')\right),
		\end{align*}
		which gives us a nice recursive form of the gradient. Therefore the future state value function $v^\pi(s')$ can be repeated unrolled by following the same equation. 

		Before unrolling, we define the probability of transitioning from state $s$ to state $s'$ in $k$ steps under policy $\pi$ as $\rho^\pi(s\to s',k)$. Thus it follows that 
		\begin{itemize}[noitemsep,topsep=0pt]
			\item when $k=0$, obviously, we have $\rho^\pi(s\to s,0)=1$ and $\rho^\pi(s\to s',0)=0$;
			\item when $k=1$, it is easy to get the probability from the transition matrix as $\rho^\pi(s\to s',1)=\sum_a\pi(a|s)\mathcal{P}_{ss'}^a$;
			\item for other cases such as going from state $s$ to $s'$ in $k+1$ steps, we can consider a middle state $x$ where it takes $k$ steps from $s$ to $x$, thus we have a recursively expression as $\rho^\pi(s\to s', k+1)=\sum_{x}\rho^\pi(s\to x, k)\rho^\pi(x\to s', 1)$.
		\end{itemize}
		We now consider unrolling the recursive representation of $\nabla v^\pi(s)$. For simplicity, let $\phi(s)=\sum_a \nabla \pi(a|s)q^\pi(s,a)$. Then it follows that
		\begin{align*}
		\nabla v^\pi(s)&=\phi(s)+\sum_a\pi(a|s)\sum_{s'}\mathcal{P}_{ss'}^a\nabla v^\pi(s')\\
		&=\phi(s)+\sum_{s'}\sum_a\pi(a|s)\mathcal{P}_{ss'}^a\nabla v^\pi(s')\tag{\(\pi(a|s)\) is irrevalent to \(s'\)}\\
		&=\phi(s)+\sum_{s'}\rho^\pi(s\to s', 1)\nabla v^\pi(s')\\
		&=\phi(s)+\sum_{s'}\rho^\pi(s\to s', 1)\left(\phi(s')+\sum_{s''}\rho^\pi(s'\to s'',1)\nabla v^\pi(s'')\right)\\
		&=\phi(s)+\sum_{s'}\rho^\pi(s\to s', 1)\phi(s') + \sum_{s''}\rho^\pi(s\to s'',2)\nabla v^\pi(s'')\\
		&=\phi(s)+\sum_{s'}\rho^\pi(s\to s', 1)\phi(s') + \sum_{s''}\rho^\pi(s\to s'',2)\phi(s'') + \sum_{s'''}\rho^\pi(s\to s''',3)\nabla v^\pi(s''')\\
		&=...\\
		&=\sum_{s'\in\mathcal{S}}\sum_{k=0}^\infty \rho^\pi(s\to s', k)\phi(s'),
		\end{align*}
		which finally arrives at
		\[\nabla v^\pi(s)=\sum_{s'}\sum_{k=0}^\infty \rho^\pi(s\to s',k)\sum_a \nabla \pi(a|s')q^\pi(s',a).\]
		Thus for the $J(\theta)$ of the episodic environment, we have
		\begin{align*}
		\nabla J(\theta)&=\nabla v^\pi(s_0)\\
		&=\sum_{s}\sum_{k=0}^\infty \rho^\pi(s_0\to s, k)\sum_a \nabla \pi(a|s)q^\pi(s,a)\\
		&=\sum_{s}\eta(s)\sum_a \nabla \pi(a|s)q^\pi(s,a)\tag{$\eta(s)\triangleq\sum_{k=0}^\infty \rho^\pi(s_0\to s, k)$}\\
		&=\left(\sum_{s}\eta(s)\right)\sum_s\frac{\eta(s)}{\sum_{s'}\eta(s')}\sum_a \nabla \pi(a|s)q^\pi(s,a)\\
		&\propto\sum_s d^\pi(s)\sum_a \nabla \pi(a|s)q^\pi(s,a),
		\end{align*}
		where the $\propto$ in the last step is incurred as $\sum_s\eta(s)$ is a constant (it is 1 in continuous case, which means the $\propto$ can be replaced by $=$ in that case), and $d^\pi(s)$ is exactly the stationary distribution. Further, such deriving also shows the connection between the two different $J(\theta)$ we defined above. Now we rewrite the gradient as
		\begin{align*}
		\nabla J(\theta)&\propto\sum_{s}d^\pi(s)\sum_a\nabla \pi(a|s)q^\pi(s,a)\\
		&=\sum_s d^\pi(s)\sum_a q^\pi(s,a)\pi(a,s)\frac{\nabla \pi(a|s)}{\pi(a|s)}\\
		&=\mathbb{E}_{\pi_\theta}[\nabla_\theta\ln \pi(a|s;\theta)q^\pi(s,a)].
		\end{align*}
		\qed\\

		For other general forms of policy gradient methods, one can refer to the paper~\cite{schulman2015highdimensional} and the note~\cite{notes_GAE} (again, many thanks to lilianweng's blog~\cite{liblog-pga}).\\

	\subsection{REINFORCE: Monte-Carlo Policy Gradient}

		We now consider the policy gradient for the case where we can get complete episodes. According to the \textit{policy gradient theorem}, it follows that
		\begin{align*}
		\nabla_\theta J(\theta)&=\mathbb{E}_{\pi_\theta}[\nabla_\theta\ln \pi(a|s;\theta)q^\pi(s,a)]\\
		&=\mathbb{E}_{\pi_\theta}[\nabla_\theta\ln \pi(a|s;\theta)G_t]
		\end{align*}
		as $q^\pi(s,a)=\mathbb E_\pi[G_t|S_t=s,A_t=a]$. Thus it is similar to \textit{Monte-Carlo policy evaluation}, we can measure $G_t$ from real complete sample episodes and use that to update our policy gradient.

		\textbf{Monte-Carlo Policy Gradient}: \textit{Starting from the state $s_0$ sampled from a distribution $d(s)$, we denote one episode as}
		\[\tau=(s_0,a_0,r_1,...,s_{T-1},a_{T-1},r_T)\sim (\pi_\theta, \mathcal{P}_{s_ts_{t+1}}^a).\]
		\textit{Then we update the parameter in the rule:}
		\[\theta\gets\theta+\alpha\gamma^tG_t\nabla_\theta \ln \pi(a|s;\theta).\]

		\begin{algorithm}[h]
		\caption{REINFORCE: Monte-Carlo Policy-Gradient Control}
		\label{alg: REINFORCE: Monte-Carlo Policy-Gradient Control}
		\begin{algorithmic}[1]
			\State initialize policy parameter $\theta\in\mathbb{R}^{d}$; 
            \State \textbf{input} a differentiable policy parameterization $\pi(a|s;\theta)$; the step size $\alpha$;
            
            \For {true}:
            	\State Generate an episode $(s_0,a_0,r_1,...,s_{T-1},a_{T-1},r_T)$;
            	\For{each step of the episode $t=0,1,...,T-1$}:
            	\State $G\gets \sum_{k=t+1}^{T} \gamma^{k-t-1}r_k$;
            	\State $\theta\gets\theta+\alpha\gamma^tG\nabla\ln \pi(a_t|s_t;\theta)$;
            	\EndFor
            \EndFor
        \end{algorithmic}
        \end{algorithm}

        The analysis for REINFORCE requires us to consider the episode level. We define the sum of rewards over the trajectory $\tau$ (for $G_t$ baseline, the case is quite similar) as
		\[R(\tau)=\sum_{t=1}^T r_t.\]

		Let $\mathcal{D}(\tau;\theta)=d(s_0)\prod_{t=0}^{T-1}\pi(a_t|s_t;\theta)\mathcal{P}_{s_ts_{t+1}}^{a_t}$ denote the probability over trajectories when executing the policy $\pi_\theta$. Then the policy gradient of $J(\theta)$ is equivalent to
		\[\begin{split}
		\nabla_\theta J(\theta)&= \nabla_\theta \mathbb{E}_{\pi_\theta}\left[\sum_{t=0}^T r_t\right]\\
		&=\sum_{\tau} \nabla_{\theta} \mathcal{D}(\tau ; \theta) R(\tau) \\
		&=\sum_{\tau} \mathcal{D}(\tau ; \theta) R(\tau) \frac{\nabla_{\theta} \mathcal{D}(\tau ; \theta)}{\mathcal{D}(\tau ; \theta)} \\ 
		&=\sum_{\tau} \mathcal{D}(\tau ; \theta) R(\tau) \nabla_{\theta} \ln \mathcal{D}(\tau ; \theta)\\
		&\approx \frac{1}{m}\sum_{i=1}^m R(\tau_i)\nabla_{\theta}\ln\mathcal{D}(\tau;\theta),
		\end{split}
		\]
		where we suppose there are $m$ episodes and refer to MC thought to approx the expectation. Such approximation is reasonable as long as $m\to\infty$.\\

		We now show that such method does not need the dynamics of the model. Considering the term that matters in the gradient we got above, it follows that
		\[\begin{aligned} \nabla_{\theta} \ln \mathcal{D}(\tau ; \theta) &=\nabla_{\theta} \ln \left[\mu\left(s_{0}\right) \prod_{t=0}^{T-1} \pi\left(a_{t} | s_{t};\theta\right) \mathcal{P}_{s_ts_{t+1}}^{a_t}\right] \\ 
		&=\nabla_{\theta}\left[\ln \mu\left(s_{0}\right)+\sum_{t=0}^{T-1} \ln \pi\left(a_{t} | s_{t};\theta\right)+\ln \mathcal{P}_{s_ts_{t+1}}^{a_t}\right] \\ &=\sum_{t=0}^{T-1} \nabla_{\theta} \ln \pi\left(a_{t} | s_{t};\theta\right), \end{aligned}\]
		and thus
		\[\nabla_\theta J(\theta)\approx \frac{1}{m}\sum_{i=1}^m R(\tau_i)\left(\sum_{t=0}^{T-1} \nabla_{\theta} \ln \pi(a_{t}^i | s_{t}^i;\theta)\right).\]
		It shows that the dynamics, \textit{i.e.} the transition matrix, of the model is not needed, which means policy gradient is a model-free method.\\
 
	\subsection{Actor-Critic Policy Gradient}

		Recall that in section~\ref{sec: value function approximation} we use $\hat{v}(s;\bm{w})\approx v^\pi(s)$. Now we combine the value approximation with policy approximation, then we will get the Actor-Critic method.

		\textbf{Actor}: \textit{Actor is actually a policy parameterization $\pi(a|s;\theta)$ to generate actions; it updates parameter $\theta$ in direction suggested by critic.}

		\textbf{Critic}: \textit{Critic is actually a value approximation $\hat{v}(s;\bm{w})$ to evaluate the reward of a state under current actor (policy); it needs to update parameter $\bm{w}$ to make accurate evaluation.}

		\begin{algorithm}[H]
		\caption{Actor-Critic}
		\label{alg: Actor-Critic}
		\begin{algorithmic}[1]
			\State initialize policy parameter $\theta$; initialize the state-value function parameter $\bm{w}$;

            \State \textbf{input} a differentiable policy parameterization $\pi(a|s;\theta)$; a differentiable state-value function parameterization $\hat{v}(s;\bm{w})$, the step size $\alpha_{\bm{w}},\alpha_\theta$;
            
            \For {true}:
        	\State Generate a start state $s$;
        	\State $I\gets 1$
        	\While {$s$ is not terminal}:
        		\State Choose action $a\gets\pi(a|s;\theta)$;
        		\State $r,s'\gets environment(s,a)$;
        		\State $\delta\gets r+\gamma\hat{v}(s';\bm{w})-\hat{v}(s;\bm{w})$;
        		\State $\bm{w}\gets\bm{w}+\alpha_{\bm{w}}\delta\nabla \hat{v}(s;\bm{w})$;
        		\State $\theta\gets\theta+\alpha_\theta I\delta\nabla\ln \pi(a|s;\theta)$;
        		\State $I\gets \gamma I$;
        		\State $s\gets s'$;
        	\EndWhile
       		\EndFor
        \end{algorithmic}
        \end{algorithm}

		As we can see, the critic is solving a familiar problem \textit{policy evaluation}, while the actor is doing \textit{policy improvement}.\\


	\subsection{Extension of Policy Gradient}

		Nowadays, State-of-the-art RL methods are almost all policy-based.

		\textbf{A2C, A3C}: Asynchronous Methods for Deep Reinforcement Learning, ICML’16. Representative high-performance actor-critic algorithm.

		\textbf{TRPO}: Trust region policy optimization: deep RL with natural policy gradient and adaptive step size.

		\textbf{PPO}: Proximal policy optimization algorithms: deep RL with importance sampled policy gradient.

\noindent\rule[0.25\baselineskip]{\textwidth}{1pt}

\pagebreak
\bibliographystyle{IEEEtran}
\bibliography{re} % 引用文件


\end{document}
